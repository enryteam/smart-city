- [智能网联汽车——深度学习与无人驾驶（二）](https://blog.csdn.net/weixin_42146017/article/details/103872248)

### 一、卷积神经网络

卷积神经网络，也叫做convert，它是计算机视觉中图像分类问题几乎都在使用的一种深度学习模型。首先给出一张用于图像分类问题的一种卷积神经网络架构的图像。

![Snipaste_2019-12-24_10-18-20.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC8xYjBjZmM0NGJjZDllODg4ZTZhNTA3NzIwMTcyNTM0Ni5wbmc?x-oss-process=image/format,png)

#### 1.输入层

对于包含两个空间轴(**高度**和**宽度**)和一个**深度**轴(也叫**通道轴**)的3D张量,其卷积也叫做**特征图**( feature map)。对于RGB图像,深度轴的维度大小等于3,因为图像有3个颜色通道：红色、绿色和蓝色。对于黑白图像(比如 MNIST数字图像),深度等于1(表示灰度等级)。

从输入层开始,卷积神经网络通过不同的神经网络结构将上一层的三维矩阵转化为下一层的三维矩阵,直到最后的全连接层。

#### 2.卷积层

1）全连接层和卷积层的根本区别在于，全连接层从输入特征空间中学习到的是全局模式（涉及所有像素的模式），而卷积层学到的是局部模式，对于图像来说，学到的就是输入图像的二维小窗口中的边缘、纹理等。

![1.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC83MDcyOWE1YmY1YzZhY2Q3ZDg5Y2EzNWYxMmVkNTE2Yy5wbmc?x-oss-process=image/format,png)

2）这个重要特性使卷积神经网络具有以下两个有趣的性质。
 卷积神经网络学到的模式具有**平移不变性**。卷积神经网络在图像右下角学到某个模式之后,它可以在任何地方识别这个模式,比如左上角。对于密集连接网络来说,如果模式出现在新的位置,它只能重新学习这个模式。这使得卷积神经网络在处理图像时可以高效利用数据。它只需要更少的训练样本就可以学到具有泛化能力的数据表示。
 卷积神经网络可以学到**模式的空间层次结构**。第一个卷积层将学习较小的局部模式(比如边缘),第二个卷积层将学习由第一层特征组成的更大的模式,以此类推。这使得卷积神经网络可以有效地学习越来越杂、越抽象的视觉概念。

![2.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC8wZGVlNmQ5ODA4NGNlNDI1NjFmM2YxNWQ1ZmIwZjg0Yi5wbmc?x-oss-process=image/format,png)

视觉世界形成了视觉模块的空间层次结构：超局部的边缘组合成局部的对象，比如眼睛或者耳朵，这些局部对象又组合成高级概念，比如“猫”。

3）**卷积运算**从输入层中提取图块,并对所有这些图块应用相同的变换,生成**输出特征图**。该输出特征图仍是一个3D张量,具有宽度和高度,其深度一般为**过滤器**的数量。比如在MNIST示例中，第一个卷积层接收一个大小为（28,28,1）的特征图，经过第一层的卷积运算后，输出一个大小为（26,26,32）的特征图，即它在输入上计算32个过滤器。

**4）注意：**

卷积由以下两个关键参数所定义。

- 从输入特征图中提取的图块尺寸：这些图块的大小通常是3x3或5x5，比如MNIST示例使用的是3x3。
- 输出特征图的深度：卷积所计算的过滤器的数量。比如第一层的深度为32。

输出的宽度和高度可能与输入的宽度和高度不同。不同的原因有两点。

- 边界效应，可以通过对输入特征图进行填充来抵消。（参数padding=valid表示不使用填充，参数padding=same表示使用填充，即填充后输出的宽度和高度与输入相同。默认值是不使用填充）
- 使用了步幅（stride）。

综上，经过一层卷积层计算的输入输出公式为

**使用填充：**

![UTOOLS1577154394960.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC9hOTNlZThjNzVmNTM5ZThmMzMyMTY2MTk5YzFkZjQ4NS5wbmc?x-oss-process=image/format,png)

**不使用填充（向上取整）：**

![UTOOLS1577154460941.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC8yZjc0ZjRlZDg1MmViOTk4ZTU3ZWFhYmE2ZTI3MTRjZC5wbmc?x-oss-process=image/format,png)

**且卷机层输出特征图深度与该层参与运算的过滤器的数量一致。**

#### 3.池化层

1）池化层神经网络不会改变三维矩阵的深度,但是它可以缩小矩阵的大小。池化操作可以认为是将一张分辨率较高的图片转化为分辨率较低的图片。通过池化层,可以进一步缩小最后全连接层中节点的个数,从而达到减少整个神经网络中参数的目的。使用池化层既可以加快计算速度也有防止过拟合问题的作用。

2）使用最大值操作的池化层被称之为**最大池化层**,这是被使用得最多的池化层结构。使用平均值操作的池化层被称之为**平均池化层**。

3）最大池化通常使用2x2的窗口和步幅2，其目的是将特征图下采样2倍。如在第一个池化层之前，特征图的尺寸是26x26，最大池化运算将其减半为13x13。

#### 4.全连接层

在经过多轮卷积层和池化层的处理之后,在卷积神经网络的最后一般会是由1到2个全连接层来给出最后的分类结果。经过几轮卷积层和池化层的处理之后,可以认为图像中的信息已经被抽象成了信息含量更高的特征。我们可以将卷积层和池化层看成自动图像特征提取的过程。在特征提取完成之后,仍然需要使用全连接层来完成分类任务。

#### 5.Softmax层

Softmax层主要用于分类问题。通过 Softmax层,可以得到当前样例属于不同种类的概率分布情况。然后取最大概率所属的种类作为最后的结果。

### 二、深度学习网络架构模型

针对深度学习在**计算机视觉领域**的应用，涌现出了许多杰出的网络架构模型。顾名思义，计算机视觉即通过创建人工模型来模拟本由人类执行的视觉任务。

计算机视觉任务的主要类型如下：

**图像分类：**在图像分类中，给出一张原始图像，你的任务是识别出该图像属于哪个类别。

![Snipaste_2019-12-24_13-25-32.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC82YTFmYTU5MWMzMWFkMTU1YTAxYjE4NzllZDE2ZDYxYS5wbmc?x-oss-process=image/format,png)

**目标检测**：在目标检测中，你的任务是找到图像中**一个或多个物体**的各自位置。这些物体可能属于同一类别，或者各自不同。

![12.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC80OTE3ZWUzN2Q1YjM0ZWUwM2JhYmRiMDM1NzlhNTc5Zi5wbmc?x-oss-process=image/format,png)

**图像分割**：图像分割是一个稍微复杂的任务，其目标是将每一个像素映射到正确的分类。

![13.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC8yYWZmNzhlYzI0NWY2ZDY2ZTM3MjViZDc4YmE1MmVjOS5wbmc?x-oss-process=image/format,png)

#### 1.图像分类

##### 1. 1AlexNet

在2012年夺得了ImageNet图像分类的冠军，而且远远超过当年的第二名。架构有5个卷积层和3个最大池化层，使用了ReLU激活函数、dropout（随机失活）层、overlap pooling（重叠池化）、LRN(Local Responce Normalization，局部响应归一化)、数据增强及CUDA加速等技术。

![AlexNet.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC9lM2E4OTM1MTFkZTFkMDg0NWU4Njg4OTQwYmY2YzI4OC5wbmc?x-oss-process=image/format,png)

##### 1.2**VGG Net**

ILSVRC2014的亚军，结构非常简洁，反复堆叠3x3的小型卷积核和2x2的最大池化层构建。相比于 AlexNet 有更小的卷积核和更深的层级，但是参数数量太多。

![VGG Net.jpg](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC9iNzQyZDMzNGQ2N2VkMjE0YWU5YmM1ZWRkODk3MjdjYS5qcGc?x-oss-process=image/format,png)

##### 1.3GoogleNet

2014年ImageNet竞赛的冠军。该架构中，随着深度增加（它包含 22 层，而 VGG 只有 19 层），研究者还开发了一种叫作「Inception 模块」的新型方法。

![Inception.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC85M2EwYTYyMzFlYWE4NDFhYTNkMjRkNWZiOTQ1YWJjOS5wbmc?x-oss-process=image/format,png)

最终架构包括堆叠在一起的多个 inception 模块。GoogleNet  的训练过程也有稍许不同，即最上层有自己的输出层。这一细微差别帮助模型更快地进行卷积，因为模型内存在联合训练和层本身的并行训练。另一方面，Inception 网络是复杂的（需要大量工程工作）。它使用大量 trick 来提升性能，包括速度和准确率两方面。

![GoogleNet.jpg](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC9kMjBiODI0NTRkZDBkNmQwYzliOTk2YzY0OTc4MmU4YS5qcGc?x-oss-process=image/format,png)

##### 1.4**ResNet**

ILSVRC 2015年的冠军。ResNet网络主要是为了解决网络加深后梯度消失的问题，提升训练过程中的优化能力。残差网络（ResNet）包含多个后续残差模块，是建立 ResNet 架构的基础。下图是残差模块的表示图。

![Snipaste_2019-12-24_16-57-24.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC8yNGFkMWQwODcyMDhkNTVkNzg1NDYzMWNkNTcyMjQyNi5wbmc?x-oss-process=image/format,png)

类似于  GoogleNet，这些残差模块一个接一个地堆叠，组成了完整的端到端网络。深度残差网络,在网络深度上不断加深，但其网络较瘦，控制了参数数量，存在明显层级，特征图个数逐层递进，保证输出特征表达能力，没有使用Dropout，利用Batch Normalize和全局平均池化进行正则化，加快了训练速度。ResNet有多个模型，常用的有ResNet-50，  ResNet-101，ResNet-152等。

![ResNet-50.jpg](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC8wMGEzMDY3YTkyZjBjNjdiMjk4M2EyY2Q4NzUwYTFhOS5qcGc?x-oss-process=image/format,png)

##### 1.5**ResNeXt**

ResNeXt 据说是解决目标识别问题的最先进技术。它建立在 Inception 和 ResNet  的概念上，并带来改进的新架构。在《Inception-v4, Inception-ResNet and the Impact of  Residual Connections on Learning》一文中，研究者通过实验明确地证实了，结合残差连接可以显著加速  Inception 的训练。

#### 2.目标检测

#### 3.图像分割

##### 3.1分类

**1）普通分割**

将不同分属不同物体的像素区域分开。如前景与后景分割开，狗的区域与猫的区域与背景分割开。

**2）语义分割**

在普通分割的基础上，分类出每一块区域的语义（即这块区域是什么物体）。如把画面中的所有物体都指出它们各自的类别。

**3）实例分割**

在语义分割的基础上，给每个物体编号。 如这个是该画面中的狗A，那个是画面中的狗B。

##### 3.2族谱

##### 1）FCN

##### 2）DeepLab

##### **3）SegNet**

##### 4）PSPNet

##### 5）Mask-RCNN

关于各模型的具体介绍和图像分割更详细的内容可以参照这篇博客。（https://blog.csdn.net/weixin_41923961/article/details/80946586）

### 三、深度学习与无人驾驶

#### 1.基于深度学习的环境感知（目标检测）

##### 1.1.传统的检测算法多是基于特征的目标检测算法

1）基于形状和颜色进行目标检测；

2）利用梯度直方图特征和支持向量机(Support Vector Machine，SVM)算法进行结合；

3）基于不同的阈值分割灰度图，使用搜索连通区域将最大稳定极值区域作为候选区域等等。

这些算法都有一个共性:利用人们设计好的特征，提取并用于检测。人为地提取特征存在着一定的主观性，会受限于一些复杂的道路情况，且受限于算法本身。

##### 1.2近些年基于深度学习的目标检测算法

由于无需进行人工的特征设计，良好的特征表达能力以及优良的检测精度。目前，基于深度学习的目标检测算法已经超越传统检测方法，成为当前目标检测算法的主流。依据其设计思想，主要可分为两种，即基于区域提名的目标检测算法（二阶检测算法）和基于端到端学习的目标检测算法（一阶检测算法）。

**二阶段检测算法**：第一步：生成可能包含物体的候选区域（Region Proposal），第二步：对候选区域做进一步的分类和定位，得到最终的检测结果。代表是：R-CNN，Fast R-CNN, Faster R-CNN。

**一阶段检测算法**：直接给出最终结果，没有生成候选区域的步骤。代表是：Yolo，SSD。

**2）R-CNN**：目标检测里程碑之作，利用selective search算法从待检测图像中提取2000个左右的候选框，用CNN提取每个候选框的特征，得到固定长度的特征向量并送入SVM中进行分类得到类别信息，送入全连接网络进行回归得到对应位置的坐标信息。

![3.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC80MTM0ODNmOTViMWVmMGFiMTdjNzkxMjkyNTkxMmQwZi5wbmc?x-oss-process=image/format,png)

**3）Fast-RCNN**：在R-CNN的基础上，采用自适应尺度池化对整个网络进行优化，从而规避了R-CNN中冗余的特征提取操作，提高了网络识别的准确率。此外,使用感兴趣区域池化层,用以提取特征层上各个候选框的固定维度的特征表示;同时，使用 SoftMax 非线性分类器,以多任务学习的方式同时进行分类和回归。由于Fast R-CNN  无需存储训练和测试过程产生的中间值,因此其速度相较于 R-CNN 大为提升。

![4.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC82ZjU4YjQ4ZDg4NzUwZTg5MTZiZDY0OGQ4NmQ1M2VkOS5wbmc?x-oss-process=image/format,png)

**4）Faster R-CNN**：在主干网络增加了RPN网络，通过一定的规则设置不同尺度的锚点在RPN的卷积特征层提取候选框来代替Selective Search等传统的候选框生成方法，实现了网络的端到端训练。

![5.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC9iOGJhNzQxYTk2M2MzZWVlMTA3ZWViY2M2YzYyNmE5OC5wbmc?x-oss-process=image/format,png)



**5）YOLO（检测20个种类）**：简化了目标检测的整个流程,视频帧图像被缩放至统一尺度大小的图像,分为  S×S 个格子,每个格子需要预测 B个包含物体的矩形框的信息和 C 个类别的归属概率值,每个矩形框包含 4 维坐标信息和 1  维目标置信度,则每个格子输出 5×B+C 维向量。 YOLO 整合了目标判定和识别,运行速度有了极大的提高。

**结构**：20层卷积层之上加上随机初始化的4个卷积层和2个全连接层。

![9.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC9hZWVlN2UzODA2OGE4NWU4ODMwYTYwNTBjM2EzMDFmMC5wbmc?x-oss-process=image/format,png)

**优势**：检测速度快、背景误检率比 R-CNN 等要低、支持对非自然图像的检测。

**缺点**:对于密集型目标检测和小物体检测都不能很好适用。

**6）YOLO-v2:**可以在速度和准确率上进行tradeoff(折中)：在67帧率下，v2在VOC2007数据集的mAP可以达到76.8; 在40帧率下，mAP可以达到78.6。

加入当下热门的Batch Normalize（BN层的添加直接将mAP硬拔了2个百分点，这一操作在yolo_v3上依然有所保留，BN层从v2开始便成了yolo算法的标配。）以及残差网络结构外；

还针对性的训练了一个高分辨率的分类网络（把这种更高分辨率的分类网络（448x448）用到detection上，发现map提升了4% ）。

**7）YOLO-v3（检测240个种类）**:采用多尺度预测（13x13x255,26x26x255,52x52x255）及更好的backbone网络(darknet-53,可以和resnet-152正面刚)，分类损失采用binary cross-entropy损失函数替换softmax损失函数。

![10.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC80YzRkNTg4ZmQ1MTJkYWU4OTJiMTFiMTUzZDllMTFjZi5wbmc?x-oss-process=image/format,png)

**8）SSD**：由于 YOLO 网络的 S×S 网格的粗糙划分导致了回归的目标位置误差较大,SSD  借鉴了区域提名的思想作出改进,使用与 Faster R-CNN 类似的 RPN 网络,不同的是 SSD 在 CNN 的多个特征层上使用 RPN  之后再作分类和边框回归,原图上小物体的检测也能有较准确的检测结果。与 YOLO 相比,SSD  仍能保持快速的检测速度,并且改进了小物体的定位精确度。

![7.png](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC82MWY3N2M5MTg0NTIyMTg3NDc1MmMwZGQ5M2YyNWFhZC5wbmc?x-oss-process=image/format,png)

#### 2.基于深度学习的无人驾驶决策控制

以车载传感器（摄像头和激光雷达）为输入，以驾驶员操作（油门、刹车、方向盘）为输出,在深度神经网络模型的训练下，可学习优秀驾驶员的驾驶模式，将决策过程视作一个不可分解的黑箱,主要的问题是决策结果不具有逻辑可解释性，并且预覆盖所有场景困难，数据需求量大。

### 四、总结

![智能网联汽车——深度学习与无人驾驶(二).jpg](https://imgconvert.csdnimg.cn/aHR0cDovL3lhbnh1YW4ubm9zZG4uMTI3Lm5ldC9iYWVjM2EyZWNmMzdhYjdiYWRiOTFkNzIyZTk1MzMzOC5qcGc?x-oss-process=image/format,png)