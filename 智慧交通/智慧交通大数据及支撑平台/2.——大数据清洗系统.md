# 一、大数据清洗系统

## 1.1 概述

### 1.1.1 数据清洗

数据清洗是清除错误和不一致数据的过程，当然，数据清洗不是简单的用更新数据记录，在数据挖掘过程中，数据清洗是第一步骤，即对数据进行预处理的过程。数据清洗的任务是过滤或者修改那些不符合要求的数据。不符合要求的数据主要有不完整的数据、错误的数据和重复的数据3大类。

各种不同的挖掘系统都是针对特定的应用领域进行数据清洗的。包括：

1) 检测并消除数据异常

2) 检测并消除近似重复记录

3) 数据的集成

4) 特定领域的数据清洗

项目中的数据来源于数据仓库，其中数据是不完整的、有噪声和不一致的。数据清理过程试图填充缺失的值，光滑噪声并识别离群点，并纠正数据中的不一致。数据清洗的目的是为挖掘提供准确而有效的数据，提高挖掘效率。

### 1.1.2 缺失值处理

对于数据集中的数据，存在有这样两种情况：

1) 数据中有大量缺失值的属性，我们通常采取的措施是直接删除，但是在有些系统进行ETL处理时，不能直接处理大量的缺失值。

2) 对于比较重要的属性，也会存在少量缺失值，需要将数据补充完整后进行一系列的数据挖掘。

针对这两种不完整的数据特征，在数据清洗时采取了以下两种方式对数据填补：

1) 将缺失的属性值用同一个常数替换，如“Unknown”。这种方式用于处理上述的第一种数据特征的数据，先用一个替换值将空值进行约束替换。处理后的数据对后期挖掘工作没有价值会选择删除。

2) 利用该属性的最可能的值填充缺失值。对于第二种数据特征的数据，事先对每个属性进行值统计，统计其值的分布状态和频率，对该属性的所有遗漏的值均利用出现频率最高的那个值来填补。

对缺失数据进行填补后，填入的值可能不正确，数据可能会存在偏置，并不是十分可靠的。然而，该方法使用了该属性已有数据的大部分信息来预测缺失值。在估计缺失值时，通过考虑该属性的值的整体分布与频率，保持该属性的整体分布状态。

### 1.1.3 数据选择

在对数据进行第一步缺失值清理后，会考虑删除掉冗余属性、或者与挖掘关系不大的属性，这称为人工选择。属性的人工选择和数据消减是不同的，即使两者的目的都是缩小所挖掘数据的规模，但却不会影响（或基本不影响）最终的挖掘结果。都属于属性的降维，但是现有的数据消减包括：数据聚合、消减维度、数据压缩和数据块消减。而人工属性选择是物理降维方式，通过对业务的理解和相关人员的沟通，对数据集中的数据进行初步的筛选。

### 1.1.4 数据变换

数据变换是数据清理过程的第二步，是对数据的一个标准化的处理。大部分数据需要进行数据变换。

数据变换是不同来源所得到的数据可能导致不一致，所以需要进行数据变换，构成一个适合数据挖掘决的描述形式。在项目中我们进行数据转换包含的处理内容有：

Ø 属性的数据类型转换

当属性之间的取值范围可能相差很大时，要进行数据的映射处理，映射关系可以去平方根、标准方差以及区域对应。

当属性的取值类型较小时，分析数据的频率分布，然后进行数值转换，将其中字符型的属性转换为枚举型。

Ø 属性构造

根据已有的属性集构造新的属性，以帮助数据挖掘过程。很多情况下需要从原始数据中生成一些新的变量作为预测变量。

Ø 数据离散化

将连续取值的属性离散化成若干区间，来帮助消减一个连续属性的取值个数。

例如年龄字段取值大于0，为了分析的方便，根据经验，可以将用户的年龄段分成几个不同的区间：0～15、16～24、25～35、36～55、大于55，分别用1，2，3，4，5来表示。

Ø 数据标准化：不同来源所得到的相同字段定义可能不一样。

如性别有男、女来表示，需要将定义标准化，把它们的定义和取值区间统一起来。如性别定义1（男）、2（女）、3（缺失）。数据标准化过程还用来消除变量之间不同数量级造成的数值之间的悬殊差异，消除个别数值较高的属性对聚类结果的影响。

### 1.1.5 数据集成

数据集成是把不同来源、格式、特点性质的数据在逻辑上或物理上有机地集中，从而为数据挖掘提供完整的数据源。

数据集成处理需要考虑以下几个问题：

（1）来自多个数据源的数据表通过相同的主键进行自然连接，各个表中的主键要相互匹配，否则不能连接。

（2）冗余问题，这是数据集成中经常发生的一个问题，所以在连接之前对各个表中字段进行人工选择，并采用自然连接的方式，防止冗余字段产生。

（3）数据值的冲突检测，来自不同数据源的属性值或许不同，所以要检查数据表中连接字段的类型和是否有相同的记录等问题。

### 1.1.6 数据削减

对大规模的数据进行复杂的数据分析与数据挖掘通常需要耗费大量时间，所以在数据挖掘前要进行数据的约减，减小数据规模，而且还需要交互式的数据挖掘，根据数据挖掘前后对比对数据进行信息反馈。数据消减技术正是用于从原有庞大数据集中获得一个精简的数据集合，并使这一精简数据集保持原有数据集的完整性，这样在精简数据集上进行数据挖掘显然效率更高，并且挖掘出来的结果与使用原有数据集所获得结果基本相同。

数据消减的目的就是缩小所挖掘数据的规模，但却不会影响（或基本不影响）最终的挖掘结果。现有的数据消减包括：

（1）数据聚合；

（2）消减维度，通过相关分析消除多余属性；

（3）数据压缩；

（4）数据块消减，利用聚类或参数模型替代原有数据。

### 1.1.7 数据清洗评估

数据清洗的评估实质上是对清洗后的数据的质量进行评估，而数据质量的评估过程是一种通过测量和改善数据综合特征来优化数据价值的过程。数据质量评价指标和方法研究的难点在于数据质量的含义、内容、分类、分级、质量的评价指标等。数据质量评估至少应该包含以下两方面的基本评估指标：

1)  数据对用户必须是可信的。可信性包括精确性、完整性、一致性、有效性、唯一性等指标。

(1) 精确性: 描述数据是否与其对应的客观实体的特征相一致。

(2) 完整性: 描述数据是否存在缺失记录或缺失字段。

(3) 一致性: 描述同一实体的同一属性的值在不同的系统是否一致。

(4) 有效性: 描述数据是否满足用户定义的条件或在一定的域值范围内。

(5) 唯一性: 描述数据是否存在重复记录。

2) 数据对用户必须是可用的。包括时间性、稳定性等指标。

(1) 时间性: 描述数据是当前数据还是历史数据。

(2) 稳定性: 描述数据是否是稳定的，是否在其有效期内。

高质量的决策必然依赖于高质量的数据，因此，数据变换操作，如规范化和集成，是导向挖掘过程成功的预处理过程，是十分必要和重要的。

### 1.1.8 在交通领域的应用

在交通领域，高质量的交通流数据是大数据及支撑平台做出正确计算的基础。前端感知设备如线圈，采集的数据量大而且采集周期短，采集的交通信息数据通常为海量数据，及时、全面、可靠的获得能够体现道路交通状况的数据，并对海量的数据进行快速、有效、深入的分析，使得海量性的动态交通数据成为ITS数据的主体，可以为实时交通控制，交通管理、交通规划等工作提供基础数据。由于前端感知设备处于非正常工作状态以及传输设备故障、通信设备故障、环境因素的变化等原因，采集得到的数据不可避免的存在着若干种质量问题，有问题的数据会影响数据分析的结果，因此有必要进行数据清洗，以提高数据集的质量，进而帮助提高后续分析的有效性和准确性。

由于环境因素异常、设备故障、通信故障等原因，交通检测器获得的数据存在无效、冗余、错误、丢失、噪声、时间点漂移等现象，通常称这些数据为“脏数据”，为了避免这些数据被分析处理，成为制约后续计算的瓶颈，影响交通状态的估计、预测和评价的效果，需要对这些数据进行消除噪声、修正错误信息、约简冗余数据、推导计算缺失数据等清理，从而提高大数据及支撑平台数据的质量，这个过程称为数据清洗，即从海量原始数据中使用一系列的逻辑检测出“脏数据”并修复或者丢弃之。

对于单个数据源，数据中包含的质量问题有如下几种：

数据无效。交通检测器获得的数据中存在突变点或不符合交通流三要素相互关系，可根据阈值理论和交通流理论判定数据的有效性，由于数据无效性的概念比较模糊，比较不易判断，也可将无效数据归类到错误数据中。

错误数据。当交通检测器或者传输线路出现故障时，采集到的数据通常是错位的，这些数据不在期望的范围内或不满足已有的原理和规则，比如车流量较少时，却有较高的车道占有率，显然是错误的数据

冗余数据。就单检测器而言，冗余数据主要是指检测器检测到的数据构成集合中的相似重复数据。从狭义的角度来看，检测器检测到的数据构成数据集，如果其中两条记录在某些交通参数上的值相等或足够相似，则认为这两条记录互为近似重复，即存在冗余数据。对多检测器而言，为了获得城市交通系统运作的完整数据，城市路网的相应路段需要布设检测器，但由于同一路段或相邻路段检测器布设密度过大，直接影响了所采集的交通数据的有效性和准确性，造成部分车辆信息重叠，具有冗余性。

数据丢失。由于检测器扫描频率不固定、传输设备或存储设备等出现故障，人员操作失误、车辆过度密集造成检测器无法正确检测车辆等原因使采集到的动态交通数据无法严格的按照指定的时间间隔上传，出现数据丢失的现象。

时间点偏移。交通检测器工作的不稳定性，设置的采集间隔经常会发生漂移，比如设置交通检测器输出数据的时间间隔为300s，而实际的间隔小于300s，如为299s，298s等，造成获得的数据并非标准时间点应该获得的数据，同时由于网络传输的原因，没有规律可循，经过长时间的积累，实际时间点体系与标准时间点体系无法匹配，带来较大的误差。

## 1.2 数据清洗方法

交通检测器获得的数据经常存在无效、冗余、错误、丢失、噪声、时间点漂移等现象鉴于错误、丢失、冗余等现象出现的比较频繁，这里主要介绍这几种问题数据的清洗方法。

### 1.2.1 错误数据的判别和修正

#### 1.2.1.1 概述

对交通流错误数据的检测大体可以分为两个层次：一是微观层次，是针对微处理器扫描和处理检测器脉冲过程中的过程进行错误鉴别；一类是宏观层次，本系统主要侧重改层次的说明，其中阈值方法、基于交通流机理的方法、阈值理论和交通流理论相结合的方法应用比较广泛，聚类分析方法、神经网络方法及重复和连续性算法使用的比较少，错误数据的检测方法如下图所示：

![image-20210727142311535](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727142311535.png)

Ø 阈值方法

阈值方法根据交通流参数在一定时间间隔内的取值必然在一个合理的数值区间范围内的原理，认为超出了这个区间，则此条记录包含错误，可见阈值的确定极为重要，因此对于该方法的研究主要集中在阈值的确定问题上，除了对流量、速度、占有率的取值进行阈值控制外，还对平均有效车辆长度进行阈值控制。

Ø 基于交通流机理的方法

这类方法以交通流理论为依据，通过交通流三个参数之间的关系判断数据是否符合交通流理论中的既定规律

Ø 阈值理论和交通流理论相结合的方法

这类方法从交通流理论的角度出发，得出阈值形式的判别规则，实现对错误数据的识别和判断。

Ø 其他方法

K.M聚类方法对交通流数据进行分析，聚类分析方法将一天的流量、速度、占有率曲线分成若干具有相同交通特性的时间段，然后在这些小时间段中根据交通特点寻找隐含的错误数据，但该方法需要结合相应的事件探测信息确定错误点。

对错误数据修正方法如下图所示

![image-20210727142325078](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727142325078.png)

#### 1.2.1.2 错误数据判别模型

错误数据判别的原则是：尽可能的保留原始数据的特征，最大限度的降低判别率。目前对错误数据的判别方法主要是针对数据自身特征而开发的，可以分为3类，分别是阈值方法、基于交通流理论的方法以及阈值理论和交通流理论相结合的方法。根据已有的理论基础，本系统的错误数据判别模型由三个层次构成：

Ø 根据数据错误往往表现为孤立点的特征，标注出采集到的数据源中的孤立点，尽可能的保留原始数据的特征

Ø 根据交通流数据段特性，将各交通流参数投射到二空间，基于“边界点与坐标轴构成的区域以外的数据极可能为错误数据”的理论，对孤立点数据进行检查，确定数据边界，最大限度的降低误判率。

Ø 结合阈值理论，交通流理论对已识别出的伪错误数据进行最终的判别并将数据剔除。

##### 1. 孤立点检测算法

在交通流领域，检测到的数据为高纬度数据，含有多个属性，以占有率、速度和流量三个属性为例，若采用传统的孤立点检测算法，对于同一个目标多属性的数据集而言，只能逐一检测每一个属性，增加了时间复杂度，同时也割裂了三属性间的关联，因此，本系统选取了相似系数和的孤立点检测算法。

假设域![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps15.jpg)为要检测的数据对象，每个对象有m个属性，即

![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps16.jpg)

为了降低误差，对输入数据在[.1,1]区间做归一化处理，待找出孤立点后再将其返还到原始区间，归一化后的X记为![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps17.jpg),用矩阵表示为：

![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps18.jpg)

为了判断X中各对象的离散程度，计算归一化处理后各对象两两之间的相似系统![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps19.jpg)，并构成相似系数矩阵

![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps20.jpg)

其中，![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps24.jpg)

令![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps27.jpg)，![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps28.jpg)是相似系数矩阵第i行的和，该值越小，就说明对象i与其他对象的距离越远，即孤立点集的候选项。

![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps29.jpg)

![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps30.jpg)为阈值，![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps31.jpg)的对象则被认为是孤立点集。

##### 2. 边界检测算法

边缘反应了图像局部区域内特征的差别，边界点区域以外的数据极有可能为错误数据，本系统采用能量边界检测算法确定边界点。

![image-20210727142704815](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727142704815.png)

![image-20210727142713985](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727142713985.png)

##### 3. 阈值理论与交通流理论的组合检测算法

并非所有的孤立点以及能量边界检测得到的边界点构成的区域以外的数据都是错误的数据，所以还需要结合阈值理论和交通流理论对上述伪错误数据进行筛选。

不同道路等级、性质、控制类型及相关交通参数的不同，使得根据交通流理论和阈值理论制定的具体评价标准不尽相同，判别规则如下表所示：

![image-20210727142827700](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727142827700.png)

需要说明，检测器获得的数据被确定为错误数据后，被剔除，不存在于数据列中，但数据顺序和总个数不变，因此下一步需要对错误数据进行修正。

#### 1.2.1.3 错误数据修正模型

##### 1. 概述

数据存在错误是一个随机事件，其本身具有相当大的偶然性和模糊性，具备信息不确定性的特点，它的发生是道路检测器多种因素联合效应作用的结果，如果把交通检测器检测到的数据作为一个系统来看，正确数据是已知的，即为“白色”信息；错误数据被剔除后，这些数据的真实值是未知的，即为“灰色”信息，因此，可将交通流数据系统看作一个灰色系统，利用灰色系统的理论进行研究。错误数据灰色修正方法的实质就是寻找检测器检测到的序列数据间的动态关系，将错误数据的修正值作为交通流数据系统的行为特征量来处理，将修正值看成是交通流数据系统这个灰色系统的灰色量。

##### 2. 灰色GM（1，1）模型

GM（1，1）模型是由一个包含单变量的一阶微分方程构成的模型。其一般形式为：

![image-20210727142922979](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727142922979.png)

![image-20210727142930881](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727142930881.png)

#### 1.2.1.4 处理流程

对错误数据的修正事实上分为两步进行，首先需要判断数据是否为错误数据，待确定错误数据后再进行修正，具体步骤如下图：

![image-20210727142948487](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727142948487.png)

Ø 孤立点检测，检测出数据集中的孤立点

Ø 边界检测，是否为边界外区域数据

Ø 判断孤立点数据以及边界外区域数据是否符合交通流、阈值理论，确定错误数据集

Ø 应用灰色GM（1，1）模型对数据进行修正

### 1.2.2 丢失数据补齐

#### 1.2.2.1 概述

由于检测设备、传输设备等的连续运行，使得通过采集器采集得到的原始数据中经常存在数据丢失的现象，由此带来的问题主要有：数据中的交通流信息不能被完全的提取、后续的交通流数据分析与深层次的数据挖掘复杂以及容易产生偏差。如果使用补齐值代替丢失值进行分析，则丢失值的准确性会直接影响分析结果。因此为了保证数据分析和处理的正确性及有效性，确保提供有效的数据，对丢失值进行正确的处理是数据清洗过程中的核心内容之一，对实现先进交通管理系统有着重要的意义。

数据丢失的原因和特性，将有助于数据补齐工作的开展，也将为后期的数据管理及融合算法提供参考，数据丢失的原因与丢失数据的特点如下表所示：

![image-20210727143041193](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727143041193.png)

对数据丢失的总体情况的识别可以采用如下方法：将一定时间内得到的诗句定义为某一时段的数据，然后对数据的时段进行扫描和判断，如果某一时段内没有得到数据，或一个时段内得到多于一组的数据，则认为该时段的数据丢失，需要进行数据补齐处理。

关于数据补齐，有两条重要的原则，一是基础数据完整性原则，采集到的原始数据保存时不应该做修改或调整，以保证足够的未经修改过的基础数据用于数据补齐，且补齐数据与原数据应分别存储；二是补齐的流程的真实性原则，通过文档记录补齐的整个操作流程，有助于增强补齐工具的透明度以便于取舍。对丢失数据的修正方法如下图所示：

![image-20210727143049288](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727143049288.png)

目前国内采用一下6种方法实现对丢失数据的补齐，不同的方法有着不同的优缺点和适用性，具体如下表所示：

| **交通流丢失数据补齐方式**                                | **缺点**                                                     |
| --------------------------------------------------------- | ------------------------------------------------------------ |
| 采用前一天的历史趋势数据进行补齐                          | 若发生交通一场情况，大大降低了估计精度                       |
| 采用历史趋势数据与（t.1）时段实际数据的加权估计值进行补齐 | 地区和时段的不同，权值应该有所不同，同时人为设定权值缺乏科学依据 |
| 采用相邻时段数据的平均值进行补齐                          | 当相邻周期数据仍存在丢失时，无法采用此方法                   |
| 由相邻路段数据得到的估计值                                | 在检测器相隔较远时存在一定的时滞性                           |
| 线性差值法                                                | 在理论分析中简单方便，但在实际计算中尤其是有大量丢失数据存在时计算复杂 |
| 将存在丢失数据的记录删掉                                  | 丢失数据量较大时，无法准确描述交通流运行特征，对交通模型的运行效果不利，不识严格意义上的丢失数据的补齐 |

从上表中看出，可能是受在线应用要求简便、可行以及数学模型发展的限制，上述方法均是采用同一属性已有的完整数据补齐丢失数据，并存在地区、时段不同，权值无法确定，导致估计精度降低等问题。所以本系统中引用粗集理论和支持向量机，可不同程度上解决上述问题。

#### 1.2.2.2 基于粗集理论的交通流丢失数据补齐

粗集理论是波兰学者提出的一种研究不完整、不确定性知识和数据的表达、学习归纳的理论方法，它能有效的分析和处理不精确、不一致或不完整的各种不完备信息系统，能使具有丢失值的对象与信息系统的其他相似对象的属性值尽可能保持一致，即可以利用相似对象属性值的不可分辨关系对丢失数据进行补齐。

假设交通检测器获得的基本交通流信息组成的系统称为基本交通流信息系统，存在丢失数据的交通流信息系统称为不完备的基本交通流信息系统。

对丢失数据的补齐要求尽可能反映含丢失数据的不完备信息系统所反映的基本特征以及隐含的内在规律。而基于粗集理论的不完备数据分析方法的基本思想正是，丢书数据值的补齐应使完整化后的信息系统所产生的分类规则具有尽可能高的支持度，产生的规则应尽量集中，换言之，是使具有丢失值的对象与信息系统的其他相似对象的属性值尽可能保持一致，使属性之间的差异尽可能保持最小。

![image-20210727143127306](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727143127306.png)

![image-20210727143138131](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727143138131.png)

![image-20210727143151929](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727143151929.png)![image-20210727143202712](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727143202712.png)

#### 1.2.2.3 基于最小二乘支持向量机的交通流丢失数据补齐

支持向量机（SVM）建立在统计学习理论基础之上，具有完备的统计学习理论基础和出色的学习性能，能较好解决小样本、非线性、高维数和局部极小点等实际问题。

SVM方法的基本思想是以结构风险最小化原则为理论基础，通过某种特定的非线性映射把样本空间映射到一个高维乃至无穷维的特征空间，并在特征空间中寻求最优划分或回归线性超平面。把此平面作为分类决策面，从而解决样本空间中的高度非线性分类和回归等问题。SVM的最终决策函数只由少数的支持向量所决定，即具有稀疏性。计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这个某种意义上避免了“维数灾难”现象的发生。

![image-20210727143230387](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727143230387.png)

![image-20210727143243392](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727143243392.png)

### 1.2.3 冗余数据简约

#### 1.2.3.1 概述

理想情况下，对于现实世界中的一个实体，数据集中应该只有一条对应的记录。但由于检测器调试不正确，同一路段检测器布设过多等原因，检测器输出的数据集极易存在信息冗余的问题，造成动态交通流数据大幅增加，不利于关键交通信息的凸显，甚至可能导致建立错误的数据挖掘模型，对后续的决策分析产生很大的影响，因此，需要对冗余数据进行约简。

为了减少数据中的冗余信息，冗余记录的识别是一个关键的步骤，本系统采用等级分组的方法，根据等级法计算每个交通参数的权值及设计多趟查找方法，提高了识别精度，同时采用分组法，降低了时间复杂度。

#### 1.2.3.2 基于等级分组法的冗余数据识别

![image-20210727143354338](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727143354338.png)

![image-20210727143408611](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727143408611.png)

![image-20210727143420281](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727143420281.png)

Ø 等级法计算权值

本系统采用RC等级转换法计算各交通参数的权值，等级法是一种计算各记录参数权重的方法，其思想为：首先各用户根据实际经验为各个交通参数制定等级，即最重要参数的等级指定为1，第二重要的参数等级制定为2等，然后根据公式（1）计算各参数的最终统一登记，最后根据公式（2）和公式（3）再计算它们相应的权重。

Ø 数据分组

交通数据不断被检测得到，构成海量数据库，为提高冗余数据的识别效率，需对大数据集作一定处理，根据分组思想，把大的数据集分割成很多不相交的小数据集，然后在各个小数据集中查找冗余数据，为提高识别率，实行多趟查找，其基本思想为：

1、首先选择能明显区别记录间特征的交通参数，把大数据集分割成很多个不相交的小数据集，不同领域数据集大小的判断标准不同，就交通检测器检测到的记录数而言，对检测器同一天的检测记录，由于不同检测器采样间隔不同，检测到的数据记录条数相应不同，采样间隔越短，记录条数越多，数据集越大，反之亦然。例如数据库中有若干天的数据，可取日期作为分割依据，把大数据集分割成数个不想交的集合。

2、分割后，若某个数据集仍然十分庞大，则选择另外关键参数，对这些数据集再次分割，如每天有24个小时，构成的数据集仍然庞大，则对这些数据集进行二次分割，取时间段，把比较大的数据集再次分割成数个小数据集。

3、若有些数据集仍然很大，可重复第二步，直到数据集分割比较合理为止。另外，引入多趟查找技术，即把数据集划分成合理的小数据集，并查找冗余记录，这一轮结束后，在选定另外关键参数或关键参数某些位，重新对数据集进行划分，并查找相似重复记录，根据实际情况决定是否进行下一轮划分查找，直至结果满意。

Ø 冗余数据的约简方法

对冗余数据的约简采用两种方法，当记录完全重复时，删除多余重复记录，只保留一条记录，当记录相似时，对流量、速度、占有率等各交通参数数据取平均值，最终只含一条约简记录，以流量为例。假设Y为约简后的结果，![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps35.jpg)为相似冗余记录，l为相似冗余记录条数，则约简后结果表示为：

![](https://gitee.com/er-huomeng/l-img/raw/master/img/wps35.jpg)

#### 1.2.3.3 处理流程

对冗余数据的处理分为两步：识别数据是否为冗余数据，待确认冗余数据后再进行约简，具体步骤如下图：

![image-20210727143519079](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727143519079.png)

### 1.2.4 基于Hadoop的分布式数据清洗

#### 1.2.4.1 概述

当前，传统的数据清洗面临的主要挑战在于：

l 随着采集设备的增多，采集的交通数据越来越多，数据的错误率也相应增多，使得展现正确数据的能力远远低于用户的需求，故必须提高数据清洗的速度和效率。

l 传统的数据清洗算法在面对海量数据表现出来的性能较低、计算能力往往不如人意，在扩展性、强壮性、伸缩性等方面也较差，这使得传统的数据清洗系统无法扩展其自身的大数据集清洗能力。

l 对应多种数据源的多样的数据格式，在合并清洗的过程中需要保证数据格式的统一性，从而达到很好的处理能力。

海量的数据处理能力、异构数据的兼容处理能力，这是基于 Hadoop 分布式数据清洗方案必须解决的问题。利用 Hadoop 生态圈的相关技术及本身核心的集群特性、强大的存储能力及计算能力、灵活的扩展伸缩性，可以很好的解决以上提出挑战问题。

#### 1.2.4.2 数据清洗技术中孤立点算法

##### 1. 孤立点简述

孤立点发现是数据挖掘的一个重要研究方面，被用来发现数据集中较小的模式，即数据集中明显不同于其他数据的对象。孤立点的研究非常重要，主要原因如下：

Ø 它对数据分析的结果有很大的影响。

Ø 虽然它通常是记录或测量的错误，但有些错误可能代表相应应用领域中有意义或感兴趣的潜在知识。

Ø 孤立点的挖掘确定经常能够导致新的知识的发现。

由于孤立点这一概念相对复杂，故不同的研究人员对其的定义也不尽相同。Knorr等对数据集 O 中的孤立点为：在 O 中存在与 O 的距离大于具体距离 D 那一小部分数据。然而，Ramaswamy 等人认为，孤立点就是与其他别的数据相比距离最大的那些数据。Breunig 等认为孤立点就是一个给定的邻域内具有最大局部孤立因子的那些数据。局部孤立因子就是在一个具体的点周围对象的距离或密度的度量。高局部孤立因子意味着与邻居很远，而低局部孤立因子则意味着邻居更近。因此当前还没有一个被广泛接受的定义。然而 Hawkins 给出的描述几乎成为被广泛接受的定义：“孤立点是在数据集中偏离大部分数据的数据，使人怀疑这些数据的偏离并非由随机因数产生，而是产生于完全不同的机制”。后来研究者根据对孤立点存在的不同假设，发展了很多孤立点检测算法，大体可以分为基于统计的算法、基于距离的算法、基于密度的算法、基于偏离的算法以及基于关联的算法等。

孤立点检测的问题就在统计学领域得到广泛研究。基于统计的孤立点算法的思想来自于统计学方法，对给定的数据集预先假定的一个分布或概率模型，然后根据模型采用不一致检验来确定孤立点。

##### 2. 基于统计的孤立点算法

孤立点检测的问题就在统计学领域得到广泛研究。基于统计的孤立点算法的思想来自于统计学方法，对给定的数据集预先假定的一个分布或概率模型，然后根据模型采用不一致检验来确定孤立点。

这类检测方法假设样本空间中所有数据符合某个分布或者数据模型，然后根据模型采用不和谐校验（discordancy test）识别离群点。不和谐校验过程中需要样本空间数据集的参数知识（eg:假设的数据分布），分布的参数知识（eg:期望和方差）以及期望的离群点数目。

不和谐校验分两个过程：工作假设和备选假设。工作假设指的是如果某样本点的某个统计量相对于数据分布的是显著性概率充分小，那么我们则认为该样本点是不和谐的，工作假设被拒绝，此时备用假设被采用，它声明该样本点来自于另一个分布模型。

如果某个样本点不符合工作假设，那么我们认为它是离群点。如果它符合备选假设，我们认为它是符合某一备选假设分布的离群点。

基于统计分布的离群点检测的缺点：

Ø 在于绝大多数不和谐校验是针对单个维度的，不适合多维度空间；

需要预先知道样本空间中数据集的分布特征，而这部分知识很可能是在检测前无法获得的。

##### 3. 基于距离的孤立点算法

引入基于距离的孤立点定义和挖掘方法，以解决统计学方法带来的一些缺陷。其定义如下：

基于距离的孤立点 DB(p,d)：如果数据集合中至少有 p 部分对象与对象 o的距离大于 d，则对象 o 是一个带参数 p，d 的基于距离的孤立点，及 DB(p,d)。

基于距离的孤立点检测算法与基于统计的方法相比有几个优点：

Ø 首先，它不需要用户知道数据集服从哪种统计分布模型。实际上，对于恰当定义的p 和 d，一个可以被给定的不一致检验检测出的孤立点同样可以利用 DB(p,d)检测出。

Ø 其次，它能够检测多种属性的孤立点，克服了基于统计的孤立点检测的这一相关缺点。该方法不仅包括基于统计模型的孤立点的相关含义，而且在挖掘相对高维数据集具有较好的效果。

Ø 它也存在两个主要问题：

Ø 距离函数和参数的选择；

Ø 仅能发现全局孤立点，而对于局部孤立点却无法很好的识别发现。

Ø 要求数据分布均匀，当数据分布非均匀时，基于距离的离群点检测将遇到困难。

##### 4. 基于密度的孤立点算法

基于密度的局部离群点检测不同于前面的方法，它不将离群点看做一种二元性质，即不简单用 Yes 或 No 来断定一个点是否是离群点，而是用一个权值来评估它的离群度。它是局部的，意思是该程度依赖于对象相对于其领域的孤立情况。这种方法可以同时检测出全局离群点和局部离群点。通过基于密度的局部离群点检测就能在样本空间数据分布不均匀的情况下也可以准确发现离群点。

##### 5. 基于偏离的孤立点算法

与以上提出的几种孤立点算法不同，基于偏离的孤立点检测是通过检查一组对象的主要特征来确定孤立点，而非采用距离量度或统计校验。若一个对象与给定的特征描述具有过分“偏离”，则判定该对象是孤立点。该算法的主要检测方法有以下两种技术，一种是采用数据立方体的方法；一种是序列异常技术，通过顺序的比较一个集合中的对象来检测。

##### 6. 基于关联的孤立点算法

以上叙述的各种孤立点检测算法都只能适用于连续属性数据集，但对于离散属性数据集并不适用，其中的主要原因是很难对离散属性数据进行求和、求距离等数字运算。

基于关联的孤立点算法其核心是通过发现频繁项集来进行孤立点检测。其思想非常简单：由关联规则算法发现的频繁模式反映了数据集中的“普遍模式”，这就是说，孤立点就是不包含频繁模式或只包含极少频繁模式的数据。即频繁模式不会包含在作为孤立点的数据中。

#### 1.2.4.3 设计思想

基本的设计思想是：对于海量数据清洗过程中需要巨大计算能力的各个模块的计算和存储扩展到 Hadoop 集群的各个节点，充分利用 Hadoop 集群强大的计算、存储能力，来进行海量数据清洗工作，提高数据清洗的并行性和准确性。对此，本系统采用分层的设计思想，在底层，通过 Hadoop 作为数据格式统一的存储平台，将各种异构数据源的数据统一到 Hadoop 的存储系统当中，并用Hadoop 来分析处理巨大的待清洗数据；在Hadoop 层之上，则为相应的并行核心清洗模块，包括数据加载模块、分布式孤立点挖掘模块、结果分析及存储模块透明的调用 Hadoop 底层的计算和存储能力。

概括来说，设计过程主要考虑的包括两点：

Ø 存储

在整个方案中，本系统可以使用 HDFS 来存储文件和原始数据。HDFS 其高数据吞吐量、强容错性等特点，都可以为海量数据清洗提供效果保证。HDFS 提供了多种访问接口，可以通过 API 也可以通过操作命令简单的进行文件或数据的查看。基于 HDFS 文件系统，我们可以将海量的待清洗数据存入，作为一个数据源服务器，将异构数据源的各种数据导入，为数据清洗引擎提供数据输入及相应的结果输出。这里需要注意的，面对异构的数据源，其多样的数据格式对数据清洗来说，是一个比较大的问题。基于此，本系统这里使用 Hive 数据仓库而不是直接使用 HDFS，来转存异构数据源的数据。对于多种数据源的数据，比方说数据库，可以使用 Sqoop 工具直接将其需要清洗的数据导入Hive 数据仓库，不仅将其存储为结构化的数据文件，同时也将其映射为一张数据表，提供完整的 sql 查询功能，通过类 SQL 语句快速实现简单的数据预处理任务。Hive 是建立在 HDFS 文件系统上的，故也继承了 HDFS 的各种优点，数据处理结果可以存入 Hive或 HDFS 中，并通过 Sqoop 写回关系数据库。

Ø 计算

对于 Hadoop 平台，其另外一个重要的部分是 Map Reduce 运行机制。在本论文提出的方案中，使用 Map Reduce，一方面将各个模块的数据交互联系起来，另一方面也可以将各个模块的计算任务发布到 Hadoop 集群中的各个计算节点。Map Reduce 具有很好的扩展性和伸缩性，它屏蔽了下层的具体运行机制，直接抽象出相应的 Map、Reduce 等编程接口供快速的实现各种算法的并行化。本论文对数据清洗引擎的核心模块——分布式孤立点挖掘模块，通过 Map Reduce 进行实现，从而提高系统的效率。在并行化的过程中，有时需要对多个 Job 进行运算控制，使用前一个 Job 的输出直接作为后一个 Job的输入。对于无法直接作为输入的 Job，则将该 Job 的输出按照指定格式定位到 HDFS文件系统，等到下一个 Job 需要使用时才从直接从文件进行读取。

#### 1.2.4.4 方案设计

根据以上基本设计思想，本系统采用基于孤立点挖掘为核心的数据清洗引擎以及底层的Hadoop 分布式系统提出一种 Hadoop 分布式数据清洗方案，以实现对错误数据(也称异常数据或孤立点数据)的分布式清洗过程。下面详细介绍该方案的框架。

![image-20210727143812536](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727143812536.png)

如上图，主要分为三大功能模块：

Ø 多源异构数据的装载预处理

Hadoop可以从任意多的数据源吞入任何类型的数据，可以是结构化数据，也可以是非结构化数据，来自多个数据源的数据可以按任何所需的方式进行合并或聚合，从而实现任意一个单一系统均无法处理的综合数据清洗及其他处理工作。这里，输入文件来自多个数据源，通过预处理，将预处理后的结果放入 Hadoop 文件系统，使得 Hadoop平台对应的作为数据源服务器，为下一步的数据清洗做准备。

Ø Hadoop 分布式计算

使用 Hadoop 分布式环境来实现集群的存储及计算，通过 HDFS 分布式文件系统实现对数据文件的存储和管理，通过 Map/Reduce 运行机制实现并行化。这里一方面实现对清洗数据的清洗，同时也负责中间输出文件的保存及管理，另一方面为下面的数据清洗引擎模块提供了分布式计算的运行机制，我们需要在 Hadoop 环境下实现算法任务的并行化处理。

Ø 数据清洗引擎

这里是整个方案核心功能实现的模块。其主要通过基于 Hadoop 的分布式孤立点挖掘算法，对整个数据集进行清洗挖掘，找出不合理的属性值，并执行相应的数据清洗动作，最终将清洗后的数据通过接口或其他方式输出。这些处理结果可以传递给任意现有的与 Hadoop 无关的企业系统做进一步处理。这里，主要包括几个子功能模块：数据加载模块、分布式孤立点挖掘算法模块、结果存储模块。依次如下：

u 数据加载模块

由于我们的数据清洗是根据属性进行清洗的。在第一步中，我们已经将所有的数据的各种属性均导入进来。这些属性分析有一些是需要分步进行的，故这里我们需要对数据进行再加载工作，将指定属性的数据加载到指定的 Hadoop 目录文件，输入数据来自于 HDFS 文件，输出数据也放于 HDFS 文件。

u 分布式孤立点挖掘算法模块

由于本论文主要是针对海量数据清洗的属性清洗的，这里采用孤立点挖掘算法，通过 Hadoop 分布式环境，找出异样的属性值，并结合相应的清洗规则继续分析处理。

u 结果存储模块

这里，我们将处理后的中间数据或最终数据结果，都通过该模块指定存放到对应的HDFS 文件系统中，同时提供接口或其他方式，将清洗后的数据用于更高层的数据处理。

根据以上的方案框架图，对应有以下的方案流程图：

![image-20210727143822728](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727143822728.png)

本分布式数据清洗方案的流程图主要包括以下步骤：

Ø 首先通过 sqoop 将各种异构数据源的数据加载到 Hadoop 分布式文件系统中。

Ø 将 HDFS 的待清洗数据、参数 N、K 作为数据清洗引擎算法的输入。数据清洗引擎根据输入，求出本轮挖掘出的 N 个孤立点(详细的数据清洗引擎流程将在后面详述)。

Ø 对于挖掘出的 N 个孤立点，则为清洗的候选点。我们需要判断这些孤立点是否满足清洗规则。若所有的 N 个点，都满足清洗规则，则将 N 个数据点都根据清洗规则进行清洗，然后将清洗后的数据写回 HDFS，重新将新的 HDFS 数据、N、K 作为新的一轮数据清洗引擎的输入；

Ø 若 N 个点只有若干个点满足清洗规则，由于孤立点的输出是根据距离优先原则，孤立性越明显，就排在越前面。故若只有若干个点满足清洗规则，则表示 N 个点中有r 个点满足清洗规则，后面的 N.r 个点则已经不满足清洗规则，此时表明已经不需要进行下一轮新的数据清洗了，因为下一轮结果出来后，前面的几个点也是这 N.r 个点。故只需要简单的清洗 r 个点，写回 HDFS，便可以结束本次数据清洗方案的过程。

Ø 若 N 个点没有点满足清洗规则，与上一步一样的分析，不需要进入新的一轮迭代数据清洗了，同时也不需要进行数据清洗操作，直接借宿本次数据清洗方案的过程。

#### 1.2.4.5 基于 Hadoop 的分布式孤立点挖掘算法

在上面的基于Hadoop的分布式清洗方案中，数据清洗引擎功能模块是整个方案的核心算法所在。这里主要针对属性值的清洗，提出了基于Hadoop 的分布式孤立点挖掘算法。该算法是针对数据量大、维度高的数据集的挖掘算法，以实现孤立点挖掘的准确性及快速性，最终保证数据清洗的性能及效率。

##### 1. 算法思想

###### ①块嵌套循环算法和基于索引算法

对于孤立点的相关挖掘算法中，对孤立点的计算，目前存在一些常见的基于距离的孤立点检测方法，这里简单列举两种方法：

（1）嵌套循环算法(NL 算法)

“NL 算法的设计思想是基于块和循环进行孤立点挖掘。假设有一个缓存，其大小是数据集的 b%，该算法就将这片缓存划分为两个相等的部分，分别标志为第一数组及第二数组。然后，算法将划分好的数据集依次放入对应的数组，对每个实体对的距离进行直接计算。对于计算结果，将每个在第一个数组内的实体的 d 邻域记录下来。某实体不是孤立点的条件是：该实体的 d 邻域中点的个数超过 n。此时，判定该实体不是孤立点，故跳出循环，对下一个点进行比较。假设 NL 算法的缓存为数据集的 50%，在这里我们将其分为 4 个逻辑块，并用 A、B、C、D 来表示，将数据集 1/4 的实体依次放入每一个逻辑块。对于每个数组的填充，则按照以下的顺序进行：

1）A 和 A，接着 A 和 B，C，D（在该步骤中，读取了 4 个逻辑块:A/B/C/D）

2）D 和 D(不需要读)，然后 D 和 A(不用读)，B，C（在该步骤中，读取了 2 个逻辑块:B/C）

3）C 和 C(不需要读)，然后 C 和 D(不用读)，A，B（在该步骤中，读取了 2 个逻辑块:A/B）

4）B 和 B(不需要读)，然后 B 和 C(不用读)，A，D（在该步骤中，读取了 2 个逻辑块:）

共有 10 块被读，相当于 10/4 次扫描数据集。其复杂度是(![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps36.jpg))，k 是数据维度(或属性个数)。其计算代价大，且随着维度的增加而增高。

（2）基于空间索引的距离计算方法

该方法是针对嵌套循环算法的计算代价大等缺点，提出的一种改进算法。该算法引入了空间索引结构(如![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps37.jpg)树)，通过将所有的点都存放在空间索引结构(这里我们以![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps37.jpg)树为例)中然后利用![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps37.jpg)树的特点，采用剪枝优化的方法，将一些必定不包含孤立点的分支剪去，如此以来，就不用对所有点进行![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps38.jpg)的计算，大大的减少了计算量。对于多维索引模式的分析说明，对于![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps37.jpg)树这一空间索引结构，一个范围查询的复杂度下限是![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps39.jpg)。当维度(或属性)k 增加时，范围查询迅速变为 O(k N)，N 是数据实体的个数。然而使用该方法查询孤立点的过程，最坏的复杂情况是![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps41.jpg)。该算法计算复杂度比嵌套循环算法降低了一个层次，不过增加了将所有的点存放入空间索引结构的时间及空间代价。这两点，在不同的场合，比重不一，故需要针对具体的情况，进行该方法的评估及选择。

针对以上两种算法的优缺点，对其进行分析。虽然进行了部分优化，但是总体而言，其计算代价仍是比较大的，因为无论如何，对于每个点 p，我们都是必须初始化![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps38.jpg)。但是由于我们的目标是找到 n 个孤立点，尤其是 n 一般是一个比较小的数，如此以来，对其他大部分点的![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps38.jpg)的计算，其实是用处不大，是可以尽量减少类似的计算量的。

基于以上分析，本系统提出一种区域划分的算法，对输入的点集进行分区，并对于非包含孤立点分区进行剪枝，以进一步减少参与计算的点。

###### ②区域划分算法

区域划分算法的核心思想是：首先对数据集合进行分区，根据一定算法，将集合划分为若干个不相交的区域，然后对区域进行判定，若该区域经判断为一定不包含孤立点的区域，则将该区域剪枝，不需去计算该区域的所有点的![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps38.jpg)。

虽然比起前面两个算法，增加了这个预分区的步骤，但是由于我们的目标值 n (即关注的孤立点数目)通常是非常小，远小于数据集合的点的总数，经过这一步，我们不需要将所有点都当成孤立点的候选点，去进行一步步的计算，反而可以将计算![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps38.jpg)的点的数量大大减小，从而提高了孤立点计算的速度。故相比起来，这一步预处理的代价远远小于其带来的计算效率的提高。这里，我们先对算法的核心思想进行简述，如下：

Ø 划分区域

在第一步中，我们使用聚类算法对数据进行聚类，然后将每个分类当成每个区域

计算每个区域所有点在![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps38.jpg)Ø 的上下限

对于每个区域 P，计算出每个区域所有点关于![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps38.jpg)上的上下限(上限用 P.upper 标记，下限用 P.lower 标记)。如此以来，对于该区域的所有点![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps43.jpg)，![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps44.jpg)

Ø 筛选出包含孤立点的参与计算的候选区域

在此步骤中，我们对区域进行剪枝，将不包含孤立点的区域去除，筛选出候选区域(候选分区，是指包含孤立点的分区)。我们在计算的过程中，记录 miniDKDist(n 个孤立点在 Dk值的下界，即 n 个孤立点的最小值)。对于每个区域，我们首先判断 P.upper 是否小于 miniDkDist。若是，则区域 P 的点都不可能是孤立点。若不是，则该区域则为候选区域。剪枝的原则就是：P.upper<=miniDKDist 的区域均不是候选区域，应该剪去，不应参与下一步计算。对于 mini Dk Dist 的计算，我们可以通过所有区域的 P.lower 来计算出来。首先，我们假定所有的分区按照 P.lower 的倒序排序。选出 P.lower 最大的 l 个分区P1,P2,…Pl，并使得分区里的所有点的数目至少为 n(n 为我们要找出的孤立点的数目)。如此，我们就可以计算出 miniDkDist：![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps45.jpg)

Ø 在候选区域中的所有点中，找出孤立点。

最后一步，孤立点将会从所有候选区域的点中计算出来。对于每个候选区域，用P.neighbors 代表 P 的邻居区域。P 的邻居区域是指与 P 的距离在 P.upper 范围以内的所有区域。当在计算区域 P 的每个点 p 的 Dk(p)值时，只有属于 P 的邻居区域的点，才参与计算。这是因为 P.upper 是区域 P 的 Dk值的上限，故只需要考虑 P 的邻居区域的点即可。同时，由于候选区域及其相应的邻居区域的点的数目将会很大，故我们在计算的过程中，可以进行采用分布式的Map.Reduce思想来计算候选区域的点，对计算点进行合理的切割，以实现计算的并行化。

##### 2. 算法流程

###### ① 区域划分

为了有效的剪枝，我们倾向于将数据集切割为这样的区域：每个区域的点是相对接近的，即相邻的点尽可能的放在同一个区域中。这思想跟聚类算法的思想类似，故使用聚类算法进行区域划分，在本算法中，是一个不错的选择。

聚类(Cluster)分析是由若干模式(Pattern)组成的，通常，模式要么是一个向量，用于标志一个度量(Measurement)，要么也可以是多维空间中的一个点。在一个聚类中的模式的相似性通常比不在同一聚类的模式更高，聚类分析正是基于此种相似性的基础进行分析的。而聚类算法，就是基于聚类分析提出的相应实现方法，一般常见的聚类算法有：分裂法 (Partitioning  Methods) 、层次法 (Hierarchical  Methods) 、基于密度的方法(density.based  methods) 、基于网格的方法 (grid.based  methods) 、基于模型的方法(Model.Based Methods)。

对于大部分聚类算法，其算法的复杂度通常是输入数据集的点的总数 N 的二次方。一般我们分析的数目 N 很大，这样的话，消耗在聚类算法上的时间和空间消耗会大大增大。故本文提出基于分布式的方案，将整个算法使用分布式化，对其实现并行化。对于聚类算法，我们提出使用基于 Hadoop 实现的 K.Means 聚类算法来实现基本的分布式聚类分析。下面简单介绍一下 K.Means 算法的核心思想：对于 K.Means 算法，首先接受输入量参数 k  ，作为待聚类的数目，接着为了使得所获得的聚类满足同一聚类中的对象相似度较高、不同聚类中的对象相似度较小的条件，算法将 n 个数据对象分割为对应的 k 个聚类，最后由各个聚类中对象的均值来确定一个对应的“中心对象”(引力中心)，并用该“中心对象”来进行计算聚类的相似度的。

因此，基于 Hadoop 实现的 K.Means 聚类算法进行区域划分的算法流程如下图所示：

![image-20210727144827730](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727144827730.png)

Ø 数据准备

初始化聚类：用随机函数生成一个原始的聚类中心描述文件，该文件作为第一轮Map 任务的输入。

数据分片：将原始数据进行数据分割，为了适合 Map Reduce 计算模型处理，须将待处理的数据记录以行记录存储，使得待处理能够进行分片，且片间数据无相关性，分片过程由 Map Reduce 运行的 Hadoop 分布式环境完成。

Ø 进行 Map 任务

Map 任务主要是完成每个点记录到原始中心点的距离的计算，并重新标记其属于的新聚类类别，其输入为待聚类所有记录函数和上一轮迭代(或初始化聚类)的聚类中心。每个 Map 函数都读取聚类中心描述文件，对于其输入的每个记录点都计算出距离其最近的类中心，并做新的类别标记(即新的类中心)，最终产生<新的类中心，记录属性变量>的中间结果。

Ø 进行 Reduce  任务

Reduce 任务主要是根据 Map 任务得到的中间结果计算新的聚类中心，供下一轮Map Reduce 的 Job 使用。Map 任务输出的中间结果作为输入数据，将所有 key 相同的记录(即有相同类别 ID 的记录)送给一个 Reduce 任务，该任务累key 相同的点个数和各个记录变量的和，求出个分量的均值，从而得到新的聚类中心文件。在以上的流程图中，为了提高 Reduce 任务的执行效率，对 Map 任务的中间结果进行分组和排序。至此，Reduce 任务结束。

Ø 聚类收敛

为了使聚类结果更为准确，使用聚类收敛的方法。当收敛聚类数几乎不再变化或震荡时可停止迭代了。在这里，我们比较上一轮 Map Reduce 的 Job 得到的聚类中心与本轮Map Reduce 的新的聚类中心距离，若变化小于给定的阀值，则算法结束，输出划分好的区域，以行记录格式输出到指定目录的 textfile 格式的文件；否则，则用本轮的聚类中心文件替换上一轮的中心文件，并启动新一轮的 Map Reduce 任务。

如此一来，我们便实现了区域划分。在这里，我们需要强调的是，我们使用聚类算法来作为一种用来更为有效、更合理合理的划分区域的方法，而不是直接用来作为孤立点挖掘的方法。虽然一些聚类算法也是可以用来作为孤立点挖掘，但是一方面这些算法所定义的孤立点含义跟我们本文提及的有一定的差别，聚类算法定义的孤立点不是非常严谨，而且其发现是伴随在这些算法进行分簇的过程中，在对孤立点挖掘的精确度也有很大偏差，故本文不采用聚类方法进行孤立点挖掘。

###### ② 区域的上下限的计算

使用聚类方法进行区域划分后，我们接下来要做的是计算区域的上下限 P.lower，P.upper，这是确定候选区域的条件之一。我们首先对其做如下定义：对于所有的点![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps46.jpg)。对于 P.lower 和P.upper 的计算，我们首先可以根据MINDIST/MAXDIST 找出离区域 P 最近的 l 个分区，使得所有分区 P1,…,Pl 中点的总数目至少为 k，则 P.lower 和 P.upper 可以根据区域 P 与这 l 个区域的 MINDIST/MAXDIST的最大值来确定。

在上个流程中已经将区域信息以行记录的形式输出到 textfile 文件中。现在我们在其基础上，在 Hadoop 分布式环境下，进行区域上下限的计算。在这里，由于区域划分的信息也是以行记录的形式进行存储的，故可以直接使用 Hadoop 分布式环境进行区域集的分片。总体流程，使用基本的Hadoop 分布式环境自动处理即可。在这里我们只需要 Map 任务，而不需要 Reduce 任务。通过 Map 任务对每个输入区域进行上下限的计算，最后输出即可。而 Map 任务的核心算法流程如下图所示：

![image-20210727144923089](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727144923089.png)

步骤1：传入的参数为存放所有区域的内存索引结构的根节点 root、参与计算的区域 P、k、mini Dk Dist。初始化局部变量 node List = root(表示节点链表)、P.lower = P.upper = ![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps47.jpg)、lower Heap(存放离区域 P 最近的 l 个区域的 MINIDIST)  、upper Heap(存放离区域 P 最近的 l 个区域的 MAXDIST)，跳至步骤 2。

步骤2：判断 Node List 是否为空。若为空，表示已经没有节点参与计算，则跳出计算，转 9；若不为空，则跳至步骤 3。

步骤3：判断 Node List 是否为空。若为空，表示已经没有节点参与计算，则跳出计算，转 9；若不为空，则跳至步骤 3。

步骤4：判断 Node 节点是否为叶子节点，若不是，则只需将 Node 节点的儿子节点添加到链表 node List 中，儿子节点的添加按照 MINDIST 的降序插入，从而保证队列的头部的MINDIST 是最大的，此后便跳至步骤 7；若是，则跳至 5。

步骤5：Node 节点为叶子节点，则遍历 Node 节点的所有区域，对于每个区域 Q，判断MINDIST(P,Q)  是否小于P.lower(初始值为正无穷大)；若条件成立，则将 Q 插入结构体lowe Heap，并根据 lower Heap 所有区域的节点数目与 lowe Heap 第一个节点(即某个区域)的点的数目之差是否超过 k。若超过 k，则将 lower Heap 的第一个节点去除，直至区域的数量在满足总点数>=k 的条件下达到最小值，从而 P.lower=MINDIST(P,lower Heap 的第一个区域)，计算结束后跳至步骤 6；若条件不成立,直接跳至步骤 6;

步骤6：该步骤用于计算 P.upper。首先判断 MAXDIST(P,Q)是否小于 P.upper(初始值为正无穷)。若条件成立，与步骤5同理，则将 Q 插入 upper Heap 结构体，并计算 P.upper。计算完后，比起步骤5，多了一步剪枝动作，判断 P.upper  是否不大于 min Dk Dist(初始值为 0)，若条件成立，则直接返回；否则，跳至步骤7。若 MAXDIST(P,Q)是否小于 P.upper 这一条件不成了，则直接跳至步骤7

步骤7：遍历 node List 中的所有节点,对于每一个节点 Node，若当前计算出来的 P.upper<MAXDIST(P,Node)  且P.lower < MINDIST(P,Node)，则表示即使该节点 Node 参与以上计算，P.upper 和 P.lower 也不会变化，故删除此节点，以减少循环次数,  跳至步骤 8

步骤8：根据步骤2迭代循环

步骤9：循环结束，将数据输出定位到分布式系统指定目录的文件。

至此，在 Hadoop 分布式环境中实现了各个区域的上下限的计算，最后将其结果定位到指定文件，供下面步骤进行读取使用。

###### ③ 候选区域的计算

这是算法的核心步骤，通过这一步，可以确定出包含孤立点的候选区域，然后将其他的区域进行剪枝，以提高孤立点的挖掘速度。

其主要思路如下：

首先使用6.4.2.4.5.2.2 计算出来的边界 P.lower/P.upper 计算孤立点的 Dk 值的下界minDkDist。

对于区域 P若其 P.upper 值大于或等于minDkDist，则区域 P 为候选区域，同时计算出 P 的邻居区域，为下一步孤立点的计算做准备。否则，该区域不包括孤立点，可以进行剪枝。

基于以上思路，基于 Hadoop 分布式环境来计算候选区域，其计算的运行机制跟前文提及的一样，仍然是使用 Map Reduce 运行机制来实现并行化。这里，这对以上两个步骤，则需要两个 Map Reduce 过程来完成对候选区域的计算。第一个 Map Reduce 任务遍历所有的区域信息以完成minDkDist 的计算，而第二个 Map Reduce 任务，以 min Dk Dist为“类全局共享变量”(Hadoop 不支持全局变量，故在第一个任务时可将其输出到一指定文件，需要使用时直接从该文件读取。)，遍历所有区域以确定候选区域。如下图：

![image-20210727145025114](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727145025114.png)

如上图所示，对于以上两个Map Reduce 任务，其算法流程如下：

1）、第一个 Map Reduce 任务：计算 miniDKDist

该 Map Reduce 任务主要是为了完成对 min Dk Dist 的计算。而对于 min Dk Dist 的计算，主要通过 P.lower 值来进行计算。对 P.lower 按照从大到小进行排序，其中区域 P1,…,Pl为前 l 个区域，且 l 个区域包括的点数至少为 n，对于满足该条件的 l 个区域，![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps50.jpg)。基于此，Hadoop 环境下对 min Dk Dist 的计算流程如下：

u 数据读取及切割

首先从上面输出文件中读取区域信息，由于区域信息是以行记录存储，故经过数据切片后，分为若干个区域分片(一个分片有若干个区域信息)。

u Map 任务

上图为 Map 任务的核心流程。遍历区域分片的每个区域 P，首先将区域 P 插入结构part Heap 中(注：是按照 P.lower 从大到小的顺序插入队列)，然后进入循环判断：判断![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps51.jpg)是否成立，若成立则删除 part Heap的第一个节点。跳出循环后，对于其他区域使用相同的流程，直至区域分片的区域均被遍历完，最后输出该区域分片的中间内存结构 part Heap。

**Reduce 任务**

![image-20210727145116648](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727145116648.png)

上图为 Reduce 任务的核心流程，具体如下：

(1).首先合并 Map 任务的输出中间结果为总的 part Heap。

(2). 读取合并后的 part Heap 队列头结点的下限值 P.lower，与 min Dk Dist进行比较。若不大于 min Dk Dist，则直接返回当前的 min Dk Dist，算法结束；反之，则循环判断删除part Heap 的头结点，直至循环跳出。

(3) 跳出循环后，继续判断当前 part Heap 区域的总点数与头结点的点数之差是否不小于n。若不小于 n，则 min Dk Dist = part Heap 头结点的 P.lower 值，并返回 min Dk Dist，算法结束；反之，直接返回 min Dk Dist，算法结束。

2）、第二个 Map Reduce 任务：计算候选区域

第二个 Map Reduce 的主要任务是对不必要的区域进行剪枝，筛选出候选区域，并计算出相关邻居区域。其主要流程如下：

u Map 任务：计算候选区域

遍历每个区域分片的各个区域 P，并读取该区域的 P.upper 及第一个 Map Reduce 任务计算出来的 min Dk Dist 结果，比较 P.upper 与 min Dk Dist。若 P.uppper > min Dk Dist，则该区域 P 为候选区域，添加到候选区域队列，并计算 P 的邻居区域。

u Reduce 任务：合并候选区域队列，并计算候选区域的邻居区域

对于以上 Map 任务的输出，我们使用一个 Reduce 任务来完成对中间输出的候选队列的合并，同时读取区域信息集合 PSet，对于每个候选区域，我们都计算其相应的邻居区域。对于 P 的邻居区域，其计算公式如下：![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps52.jpg)。最后，将候选区域集及相关的邻居区域输出，为孤立点的最终计算做准备。

###### ④ 基于候选区域孤立点的计算

这是算法的最后一步。通过以上的三个步骤，我们已经将孤立点的范围缩小到候选区域的所有点，通过最后一步，我们将把前 n 个孤立点计算出来。

与前几个步骤类似，基于 Hadoop 分布式环境来从候选区域中计算出孤立点，对候选区域进行数据切割(在6.4.2.4.5.2.3 步骤中，我们已经将结果输出为适合 Hadoop 并行处理的记录形式)，在 Map 任务中，对于每个区域分片中的候选区域所有点的 Dk值进行计算，求出每个区域分片的局部最优解：该区域分片的 n 个孤立点，然后通过 reduce 任务，按照各个点的 Dk值进行倒序排序，最后输出最终的 n 个孤立点。总体核心流程如下图所示：

![image-20210727145206360](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727145206360.png)

步骤1：对于每个候选区域，首先将候选区域及相关邻居区域的所有点插入内存索引树结构的变量，该变量用于点p 的第 k 近邻距离 Dk Dist 的计算。转步骤2；

步骤2：从6.4.2.4.5.2.2小节的输出文件中读取孤立点的 Dk下限 min Dk Dist，同时初始化局部变量out Heap，用于保存孤立点的队列结构。转步骤3；

步骤3：遍历候选区域的所有点，若候选区域的点还有未遍历的点，转步骤4；否则，转步骤8；

步骤4：此时点未被遍历完，则抽出该点 p，并设置为已遍历标志。然后对于该点 p，首先计算该点在该候选区域及邻居区域范围的第 k 近邻距离 Dk Dist 值，这里需要使用步骤1的内存索引树结构，以快速计算出结果。转步骤5；

步骤5：判断条件![img](https://gitee.com/er-huomeng/l-img/raw/master/img/wps53.jpg)条件是否成立。若成立，则转步骤6；否则，转步骤3;

步骤6：该点的 Dk Dist 值大于 min Dk Dist，表示该点可以考虑为孤立点，故将点 p 插入孤立点队列 out Heap，转步骤7；

步骤7：判断 out Heap 的点数是否超过 n，若超过 n，则将 out Heap 的头结点删除，转步骤3)；否则，则直接转步骤3；

步骤8：算法结束，返回长度为n 的孤立点队列 out Heap。