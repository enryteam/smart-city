- [邢国良：支持车路协同自动驾驶的实时边缘系统](https://mp.weixin.qq.com/s/J_QLNqKmzfMFUGRuiCoziA)

> 实时并且高效的数据融合不仅可以给自动驾驶车辆带来更大的感知范围，通过和合适的感应器进行数据融合，自动驾驶车辆全天候工作也不再是纸上谈兵。相关工作已发表于2021年的国际会议SenSys, MobiCom, IoTDI，并获IoTDI 最佳学生论文奖。	——邢国良

近年来，自动驾驶所带来的巨大的经济利益已经成为了一个不可否认的事实。为了实现由L2等级向L4等级自动驾驶的跨越，一种名为“车路协同的新型自动驾驶模式”也逐渐在学术界和工业界获得了认可。在这种新型模式下，通过赋予路灯等交通基础设施感知、通讯以及计算功能（例如给路灯安装激光雷达，WiFi收发器以及CPU），路灯所组成的边缘系统可以在车路协同的模式中扮演起辅助车辆自动行驶的重要角色。

 然而，将路灯等交通基础设施等边缘系统融入新型车路协同的自动驾驶模式中还面临着诸多挑战。首先在路灯等设施架设光纤或者赋予5G通讯功能的开销十分大，路灯所在的边缘端需要在和服务器通讯之前，先通过深度学习完成车辆/行人检测、路面测绘以及自动驾驶车辆的碰撞检测和警告等实时要求高的计算任务。但是路灯所具备的计算功能在能耗预算方面十分有限，而深度学习模型普遍需要在有高算力的设备上才能运行。同时，在运行在路灯边缘端深度学习模型存在非常高的异构性（不同的模型有着不同的准确率、延迟以及能耗要求），如何让路灯这样的边缘端同时支持运行实时的、面向不同任务的深度学习模型也成为了一个非常大的挑战。

邢教授提出了一种新型的应用范式--**混合实时深度学习任务。**在这种应用范式下边缘设备需要同时执行多种具有多样化实时性/准确性要求的深度学习任务。现有的对于混合实时深度学习的结局办法是通过对于每一个DL模型进行优化，然后把每一个模型当作一个黑盒子进行调度。然而这样的做法因为会导致模型与模型之间的资源争抢而无法将边缘端整个系统的资源进行最大化的利用。为了解决这个问题，邢教授团队提出了RT-mDL，一个新的实时深度学习框架，支持在具有异构 CPU 和 GPU 资源的边缘平台上运行具有不同实时/准确率要求的混合深度学习任务。RT-mDL包含的有如下几个部分：

**1.** **存储有界模型缩放算法**，该算法在用户指定的存储边界下生成一系列具有不同计算工作负载和精度的细粒度候选模型变体；

**2.** **基于多目标优化算法的优化框架**，旨在联合优化模型缩放和基于优先级的任务调度，以满足混合深度学习任务的不同实时/准确性要求；

**3.一个基于优先级的深度学习任务调度器**，它使用独立的CPU/GPU任务队列来大幅提高CPU/GPU的时间利用率；

**4. 基于优先级的GPU打包机制来提高GPU空间利用率**。通过在F1/10自动驾驶测试平台和三个Nvidia边缘平台的测试，邢教授团队的实验广泛表明RT-mDL可以使多个并发深度学习任务在资源有限的边缘平台上满足实时性能需求。

除了混合实时深度学习，车路协同自动驾驶的另一大关键技术就是**传感器的实时数据融合**：如何将基础设施上激光雷达（LiDAR）、摄像头以及雷达数据与车辆的感应器数据进行融合。邢教授在学术沙龙所分享的另一项工作VI-Eye正是研究了如何高效的配准基础设施以及自动驾驶车辆上LiDAR所记录的点云数据，即将车辆的LiDAR和路边基础设施的LiDAR所生成两个3D点云进行配准已合成单个点云。实现点云配准在车路协同自动驾驶有着非常重要的意义。通过点云配准，自动驾驶车辆可以利用额外的LiDAR实现更大的感知范围，并且以此带来更强大的物体检测/跟踪以及路线规划和导航能力。

现有的大部分点云配准算法由于需要将点云中的每一个点进行配准而导致高延迟进而无法满足自动驾驶的实时性要求。而邢教授团队所提出的**VI-Eye的核心思想是通过自动驾驶场景中的领域知识将所需要配准的点减少到几类特定的语义对象（例如路面、车道线、和交通标志）**。这样的做法大大的减少了配准所需要的计算要求从而来适应自动驾驶这一应用场景。另外由于所关注的语义对象常见且静态，在自动驾驶这种动态场景中也具有鲁棒性。而且因为VI-Eye允许针对车辆和基础设施独地定义语义信息，该框架也给车路协同自动驾驶带来了可扩展性。**实验表明VI-Eye相对最先进的点云配准工作精度提高了至少5倍，速度提高了至少两倍。**

实时并且高效的数据融合不仅可以给自动驾驶车辆带来更大的感知范围，通过和合适的感应器进行数据融合，自动驾驶车辆全天候工作也不再是纸上谈兵。毫米波雷达是近年来新兴的一种感应器，由于其可以全天候工作，受环境影响小，能在光线不理想时可以辅助相机的特点而备受自动驾驶界的欢迎。邢教授团队利用毫米波雷达数据以及相机数据的融合技术，提出了一种名为milliEye的数据融合系统，提高了动态场景下目标检测的鲁棒性。相比传统的融合技术，milliEye只在接近输出计算结果的时候才将训练相机数据以及毫米波数据的部分进行结合。这一方法大大降低了对于标注好的多模态数据的依赖，并且可以充分利用已有的相机数据集。在极具挑战的极低光场景下，milliEye达到了74.1%的准确度（Tiny YOLOv3的准确度仅为63.0%）。