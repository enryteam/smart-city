# 使用Keadm进行部署

Keadm用于安装KubeEdge的云端和边缘端组件。它不负责K8s的安装和运行。

请参考 [kubernetes-compatibility](https://github.com/kubeedge/kubeedge#kubernetes-compatibility) 了解 **Kubernetes** 兼容性来确定安装哪个版本的Kubernetes。

## 使用限制

- `keadm` 目前支持 Ubuntu 和 CentOS OS。RaspberryPi的支持正在进行中。
- 需要超级用户权限（或root权限）才能运行。

## 设置云端（KubeEdge主节点）

默认情况下边缘节点需要访问cloudcore中 `10000` ，`10002` 端口。

`keadm init` 将安装 cloudcore，生成证书并安装CRD。它还提供了一个命令行参数，通过它可以设置特定的版本。

**重要提示：** 1. 必须正确配置 kubeconfig 或 master 中的至少一个，以便可以将其用于验证k8s集群的版本和其他信息。 1. 请确保边缘节点可以使用云节点的本地IP连接云节点，或者需要使用 `--advertise-address` 标记指定云节点的公共IP 。 1. `--advertise-address`（仅从1.3版本开始可用）是云端公开的地址（将添加到CloudCore证书的SAN中），默认值为本地IP。

举个例子：

```shell
# keadm init --advertise-address="THE-EXPOSED-IP"(only work since 1.3 release)
```

输出：

```
Kubernetes version verification passed, KubeEdge installation will start...
...
KubeEdge cloudcore is running, For logs visit:  /var/log/kubeedge/cloudcore.log
```

## 设置边缘端（KubeEdge工作节点）

### 从云端获取令牌

在**云端**运行 `keadm gettoken` 将返回token令牌，该令牌将在加入边缘节点时使用。

```shell
# keadm gettoken
27a37ef16159f7d3be8fae95d588b79b3adaaf92727b72659eb89758c66ffda2.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1OTAyMTYwNzd9.JBj8LLYWXwbbvHKffJBpPd5CyxqapRQYDIXtFZErgYE
```

### 加入边缘节点

`keadm join` 将安装 edgecore 和 mqtt。它还提供了一个命令行参数，通过它可以设置特定的版本。

举个例子：

```shell
# keadm join --cloudcore-ipport=192.168.20.50:10000 --token=27a37ef16159f7d3be8fae95d588b79b3adaaf92727b72659eb89758c66ffda2.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1OTAyMTYwNzd9.JBj8LLYWXwbbvHKffJBpPd5CyxqapRQYDIXtFZErgYE
```

**重要提示：** 1. `--cloudcore-ipport` 是必填参数。 1. 加上 `--token` 会自动为边缘节点生成证书，如果你需要的话。 1. 需要保证云和边缘端使用的KubeEdge版本相同。

输出：

```shell
Host has mosquit+ already installed and running. Hence skipping the installation steps !!!
...
KubeEdge edgecore is running, For logs visit:  /var/log/kubeedge/edgecore.log
```

### 启用 `kubectl logs` 功能

`kubectl logs` 必须在使用 metrics-server 之前部署，通过以下操作激活功能：

1. 确保您可以找到 Kubernetes 的 `ca.crt` 和 `ca.key` 文件。如果您通过 `kubeadm` 安装Kubernetes 集群，这些文件将位于 `/etc/kubernetes/pki/` 目录中。

   ```shell
   ls /etc/kubernetes/pki/
   ```

2. 设置 `CLOUDCOREIPS` 环境。环境变量设置为指定的 cloudcore 的IP地址，如果您具有高可用的集群，则可以指定VIP（即弹性IP/虚拟IP）。

   ```bash
   export CLOUDCOREIPS="192.168.0.139"
   ```

   （警告：建议使用同一 **终端** 来保持系统工作的持续，在必要时再次键入此命令。）使用以下命令检查环境变量：

   ```shell
   echo $CLOUDCOREIPS
   ```

3. 在云端节点上为 **CloudStream** 生成证书，但是，生成的文件不在 `/etc/kubeedge/` 中，我们需要从GitHub的存储库中拷贝一份。

将用户更改为root：

````
```shell
sudo su
```
从原始克隆的存储库中拷贝证书：
```shell
cp $GOPATH/src/github.com/kubeedge/kubeedge/build/tools/certgen.sh /etc/kubeedge/
```
将目录更改为kubeedge目录：
```shell
cd /etc/kubeedge/
```
从 **certgen.sh** 生成证书
```bash
/etc/kubeedge/certgen.sh stream
```
````

1. 需要在主机上设置 iptables。（此命令应该在每个apiserver部署的节点上执行。）（在这种情况下，须在master节点上执行，并由root用户执行此命令。） 在运行每个apiserver的主机上运行以下命令：

   **注意:** 您需要先设置CLOUDCOREIPS变量

   ```bash
   iptables -t nat -A OUTPUT -p tcp --dport 10350 -j DNAT --to $CLOUDCOREIPS:10003
   ```

   > 端口10003和10350是 CloudStream 和 Edgecore 的默认端口，如果已发生变更，请使用您自己设置的端口。

   如果您不确定是否设置了iptables，并且希望清除所有这些表。（如果您错误地设置了iptables，它将阻止您使用 `kubectl logs` 功能） 可以使用以下命令清理iptables规则：

   ```shell
   iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
   ```

2. `/etc/kubeedge/config/cloudcore.yaml` 和 `/etc/kubeedge/config/edgecore.yaml` 上 cloudcore 和 edgecore **都要** 修改。将 **cloudStream** 和 **edgeStream** 设置为 `enable: true` 。将服务器IP更改为 cloudcore IP（与 $ CLOUDCOREIPS 相同）。

   在 cloudcore 中打开 YAML 文件：

   ```shell
   sudo nano /etc/kubeedge/config/cloudcore.yaml
   ```

   在以下文件中修改( `enable: true` )内容：

   ```yaml
   cloudStream:
     enable: true
     streamPort: 10003
     tlsStreamCAFile: /etc/kubeedge/ca/streamCA.crt
     tlsStreamCertFile: /etc/kubeedge/certs/stream.crt
     tlsStreamPrivateKeyFile: /etc/kubeedge/certs/stream.key
     tlsTunnelCAFile: /etc/kubeedge/ca/rootCA.crt
     tlsTunnelCertFile: /etc/kubeedge/certs/server.crt
     tlsTunnelPrivateKeyFile: /etc/kubeedge/certs/server.key
     tunnelPort: 10004
   ```

   在 edgecore 中打开 YAML 文件：

   ```shell
   sudo nano /etc/kubeedge/config/edgecore.yaml
   ```

   修改以下部分中的文件 (`enable: true`), (`server: 192.168.0.193:10004`):

   ```yaml
   edgeStream:
     enable: true
     handshakeTimeout: 30
     readDeadline: 15
     server: 192.168.0.139:10004
     tlsTunnelCAFile: /etc/kubeedge/ca/rootCA.crt
     tlsTunnelCertFile: /etc/kubeedge/certs/server.crt
     tlsTunnelPrivateKeyFile: /etc/kubeedge/certs/server.key
     writeDeadline: 15
   ```

3. 重新启动所有cloudcore和edgecore。

   ```shell
   sudo su
   ```

   cloudCore:

   ```shell
   pkill cloudcore
   nohup cloudcore > cloudcore.log 2>&1 &
   ```

   edgeCore:

   ```shell
   systemctl restart edgecore.service
   ```

   如果您无法重启 edgecore，请检查是否是由于 `kube-proxy` 的缘故，同时杀死这个进程。 **kubeedge** 默认不纳入该进程，我们使用 [edgemesh](https://github.com/kubeedge/kubeedge/blob/master/docs/proposals/edgemesh-design.md) 来进行替代

   **注意：** 可以考虑避免 `kube-proxy` 部署在edgenode上。有两种解决方法：

   1. 通过调用 `kubectl edit daemonsets.apps -n kube-system kube-proxy` 添加以下设置：

      ```yaml
      affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: node-role.kubernetes.io/edge
                  operator: DoesNotExist
      ```

   2. 如果您仍然要运行 `kube-proxy` ，请通过在以下位置添加 `edgecore.service` 中的 env 变量来要求 **edgecore** 不进行检查edgecore.service：

      ```shell
      sudo vi /etc/kubeedge/edgecore.service
      ```

      - 将以下行添加到 **edgecore.service** 文件：

      ```shell
      Environment="CHECK_EDGECORE_ENVIRONMENT=false"
      ```

      - 最终文件应如下所示：

      ```
      Description=edgecore.service
      
      [Service]
      Type=simple
      ExecStart=/root/cmd/ke/edgecore --logtostderr=false --log-file=/root/cmd/ke/edgecore.log
      Environment="CHECK_EDGECORE_ENVIRONMENT=false"
      
      [Install]
      WantedBy=multi-user.target
      ```

### 在云端支持 Metrics-server

1. 实现该功能点的是重复使用了 cloudstream 和 edgestream 模块。因此，您还需要执行 *启用 `kubectl logs` 功能* 的所有步骤。

2. 由于边缘节点和云节点的 kubelet 端口不同，故当前版本的 metrics-server（0.3.x）不支持自动端口识别（这是0.4.0功能），因此您现在需要手动编译从master分支拉取的镜像。

   Git clone 最新的 metrics server 代码仓:

   ```bash
   git clone https://github.com/kubernetes-sigs/metrics-server.git
   ```

   转到 metrics server 目录:

   ```bash
   cd metrics-server
   ```

   制作 docker 容器:

   ```bash
   make container
   ```

   检查您是否有此 docker 镜像：

   ```bash
   docker images
   ```

   | 仓库                                                  | 标签                                     | 镜像ID       | 创建时间 | 大小   |
   | ----------------------------------------------------- | ---------------------------------------- | ------------ | -------- | ------ |
   | gcr.io/k8s-staging-metrics-serer/ metrics-serer-amd64 | 6d92704c5a68cd29a7a81bce68e6c2230c7a6912 | a24f71249d69 | 19秒前   | 57.2MB |
   | metrics-server-kubeedge                               | latest                                   | aef0fa7a834c | 28秒前   | 57.2MB |

   确保您使用镜像ID来对镜像标签进行变更，以使其与yaml文件中的镜像名称一致。

   ```bash
   docker tag a24f71249d69 metrics-server-kubeedge:latest
   ```

3. 部署yaml应用。可以参考相关部署文档：https://github.com/kubernetes-sigs/metrics-server/tree/master/manifests。

   注意：下面的那些iptables必须应用在机器上（精确地是网络名称空间，因此metrics-server也需要在主机网络模式下运行）metric-server在其上运行。

   **注意：** 下面的那些iptables必须应用在已运行metric-server 机器上（精确地命名是网络名称空间，因此metrics-server也需要在主机网络模式下运行）

   ```
   iptables -t nat -A OUTPUT -p tcp --dport 10350 -j DNAT --to $CLOUDCOREIPS:10003
   ```

   （引导对metric-data的请求edgecore:10250至在cloudcore和edgecore之间的隧道中，iptables至关重要。）

   在部署 metrics-server 之前，必须确保将其部署在已部署apiserver的节点上。在这种情况下，这就是master节点。作为结果，需要通过以下命令使主节点可调度：

   ```shell
   kubectl taint nodes --all node-role.kubernetes.io/master-
   ```

   然后，在 deployment.yaml 文件中，必须指定 metrics-server 部署在主节点上。（选择主机名作为标记的标签。）

   在**metrics-server-deployment.yaml**中

   ```yaml
       spec:
         affinity:
           nodeAffinity:
             requiredDuringSchedulingIgnoredDuringExecution:
               nodeSelectorTerms:
               - matchExpressions:
                 #Specify which label in [kubectl get nodes --show-labels] you want to match
                 - key: kubernetes.io/hostname
                   operator: In
                   values:
                   #Specify the value in key
                   - charlie-latest
   ```

**重要提示：** 1. Metrics-server需要使用主机网络网络模式。

1. 使用您自己编译的镜像，并将 imagePullPolicy 设置为Never。

2. 为 Metrics 服务器启用 –kubelet-use-node-status-port 功能

   需要将这些设置写入部署yaml（metrics-server-deployment.yaml）文件中，如下所示：

   ```yaml
         volumes:
         # mount in tmp so we can safely use from-scratch images and/or read-only containers
         - name: tmp-dir
           emptyDir: {}
         hostNetwork: true                          #Add this line to enable hostnetwork mode
         containers:
         - name: metrics-server
           image: metrics-server-kubeedge:latest    #Make sure that the REPOSITORY and TAG are correct
           # Modified args to include --kubelet-insecure-tls for Docker Desktop (don't use this flag with a real k8s cluster!!)
           imagePullPolicy: Never                   #Make sure that the deployment uses the image you built up
           args:
             - --cert-dir=/tmp
             - --secure-port=4443
             - --v=2
             - --kubelet-insecure-tls
             - --kubelet-preferred-address-types=InternalDNS,InternalIP,ExternalIP,Hostname
             - --kubelet-use-node-status-port       #Enable the feature of --kubelet-use-node-status-port for Metrics-server
           ports:
           - name: main-port
             containerPort: 4443
             protocol: TCP
   ```

## 重置KubeEdge master节点和工作节点

### Master

`keadm reset`将停止 `cloudcore` 并从 Kubernetes master 中删除与KubeEdge相关的资源，如 `kubeedge` 命名空间。它不会卸载/删除任何先决条件。

它为用户提供了一个标志，以指定kubeconfig路径，默认路径为 `/root/.kube/config` 。

例子：

```shell
 # keadm reset --kube-config=$HOME/.kube/config
```

\### 节点 `keadm reset` 将停止 `edgecore` ，并且不会卸载/删除任何先决条件。