# 系统规划和平台建设

## 1.1 存储规划

### 1.1.1 数据库存储

Ø **数据库存储规模**

1) 基础数据

存储的基础数据包含城市路网数据，公交线路数据，公交车辆数据，长途客运数据，出租车数据，危化品数据，共享单车数据，火车客运数据，民航客运数据，交通资产数据，出行需求数据，公路费用数据，气象数据，监控设备数据，追逃车辆数据等。

我市主要路口数目前按400个估算，各个路口以及关联车道对应基础数据量为1KB，计为400KB。公交线路按200条估算每条线路数据为1KB，计为200KB，现有公交车辆数据为2000辆，每辆公交车辆基础数据为100字节，计为20KB。长途客运数据，出租车数据，危化品数据，共享单车数据，火车客运数据，民航客运数据现估算为 5MB，由于相关客运数据每日变动，故此数据为每日递增。交通资产数据，包含种类较多，其数据为后期交通资产管理支撑数据，为40MB。目前我市人口为350万，按照1%抽样，每个样本为200字节，故计为7MB。公路费用数据，相对稳定，且数量较少，计为1IKB。气象数据计为100B（每日变化），每个路口平均监控设备数为5个，每个监控设备基础数据为100B，故计为200KB,追逃车辆数据，故为1KB。

以上数据为基础数据相关估算，合计为50M，且相关客运数据按每日5MB数据量增长。

2) 实时计算数据

2.1) 城市交通运行数据

城市交通运行数据为交通运行指标性数据按400条主要路道计算，每条路道100字节，10分钟更新一次（24小时）。合：400 * 100 * 6 * 24 / (1000 * 1000)  = 6MB

2.2) 公交车实时位置数据

我市2000辆公交车，按每天6：00~23:00共15小时运行时间，（每秒钟都上报公交车定位消息），每条公交车行驶轨迹按100字节计算，按月存储。每天的公交车消息数量= 2000*15*60*60 = 1.08亿。每日的公交车行驶轨迹存储 = 1.08亿 *100字节/1024/1024/1024=10G

2.3) 公交(地铁刷卡)数卡数据

目前公交车辆为2000辆，未来将建成地铁，公共出行人数将大幅度增加。按照人均出行一次（一个来回）计算，每日产生的刷卡数据量为：350W * 100B * 2 = 670MB。

2.4) 长途客车实时数据

长途客车按500辆估算，每天12小时运行时间，其每日产出的数据量为 500 * 12 * 60 * 60 * 100B = 2GB

2.5) 出租车实时数据

出租车按7000辆估算，每天24小时运行时间，其每日产出的数据量为 7000 * 24* 60 * 60 * 100B = 56GB

2.6) 危化品车实时数据

出租车按100辆估算，每天12小时运行时间，其每日产出的数据量为 100 * 12 * 60 * 60 * 100B = 400 MB

2.7) 共享单车实时数据

共享单车按10W辆估算，其每日产出的数据量为 10W * 24 * 60 * 60 * 100B * 10%， (只计算每辆单车在激活状态下的位置信息) = 80GB。

2.8) 路口通行量

400个路口每个路口通行量按30分钟更新一次，其数据量为 400 * 24 * 2 * 100B = 2MB

2.9) 基于车辆识别的OD分析数据

目前我市机动车数量为100W，其车辆识别的数据量= 400*5* 1024B*12*60*60+400*1* 1024B*12*60*60=100G

2.10) 套牌车以及相关识别车辆数据

目前我市机动车数量为100W，其车辆识别的数据量= 400*5* 1024B*12*60*60+400*1* 1024B*12*60*60=100G

以上每日产生的数据量合计为：6MB +10G + 670MB + 2GB + 56GB + 400 MB + 80GB + 2MB + 8M + 100G ，约300G

Ø **数据库存储规划**

  1) IO规划

确保数据均匀地分布在所有的物理磁盘中，数据库索引表空间和数据表空间容量器应该位于不同的物理磁盘或者Raid卷，对于表空间容器和数据库日志，也应该放到不同的磁盘或者Raid卷，非极端情况下，考虑使用Raid1足够。

2) 表空间规划

根据表的访问率来映射表空间：一个数据库中大概只有40%到50%左右的表其具有比较高的访问率；将高访问量的表放在一个或几个空间中，放在性能比较高的硬盘上；或者存放在活跃程度低的硬盘中。索引数据、长字段数据、普通数据分开存放：减小竞争压力，增加每次IO读取记录数；至少也要将这个索引表空间存放在性能比较高的硬盘上；长字段，空间利用率低，单独存放，节省空间。根据备份的需要来规划表空间：备份的时候，只备份某几个表空间；20T& 2T*10。根据表的大小来规划表空间：大表，DMS；小表，SMS。

3) 表规划

减少行的长度，通过增加冗余数据，减少关联查询的需求。

4) 索引规划

避免改变列类型，Oracle 会优先考虑 number to char，尽量做到查询类型的条件匹配。

5) 高性能SQL

只查询需要的数据，拒绝大SQL（简单SQL的缓存命中率高减少锁表时间 SQl复杂，SQL优化器难以处理，无法获得较优的执行路径，业务复杂度高，出现问题难以调试，后期维护成本较高，性能一般不好）。

### 1.1.2 视频云存储

#### 1.1.2.1 云存储系统简介

随着智慧交通感知系统规模越来越大，以及高清视频的大规模应用，智慧交通感知系统中需要存储的数据和应用的复杂程度在不断提高，且智慧交通感知系统需要长时间持续地保存到存储系统中，并要求随时可以调用，对存储系统的可靠性和性能等方面都提出了新的要求。在未来的复杂系统中，数据将呈现爆炸性的海量增长，提供对海量数据的快速存储及检索技术，显得尤为重要，存储系统正在成为视频监控技术未来发展的决定性因素。

面对百PB级的海量存储需求，传统的SAN或NAS在容量和性能的扩展上会存在瓶颈。而云存储可以突破这些性能瓶颈，而且可以实现性能与容量的线性扩展，这对于追求高性能、高可用性的企业用户来说是一个新选择。

云存储是在云计算概念上延伸和发展出来的一个新的概念，是指通过集群应用、网格技术或分布式文件系统等功能，应用存储虚拟化技术将网络中大量各种不同类型的存储设备通过应用软件集合起来协同工作，共同对外提供数据存储和业务访问功能的一个系统。所以云存储可以认为是配置了大容量存储设备的一个云计算系统。

云存储系统基于云架构进行开发，采用面向用户业务应用的设计思路，融合集群应用、负载均衡、虚拟化、云结构化、离散存储等技术，可将网络中大量各种不同类型的存储设备通过专业应用软件集合起来协同工作，共同对外提供高性能、高可靠、不间断的数据存储和业务访问服务。

#### 1.1.2.2 云存储优势

Ø 高扩展存储设计

分布式存储系统采用Scale-out存储架构，将海量数据压力分散到多个并发存储节点，数据和元数据均匀分布于各个节点上，避免资源争用，系统性能（吞吐量）按照比例扩展，并且各个存储节点之间负载均衡，有效避免单节点性能瓶颈。这种架构，不仅保持了对象存储系统高可靠、高性能的优点，而且使得存储系统具有更好的扩展性。通过Scale-out架构，轻松实现容量扩展，可实现从3节点到288节点的轻松扩容。

分布式存储系统能够根据业务增长进行平滑扩容，系统每增加一个存储节点，都能够自动识别，将该存储空间加入到整系统中，并根据负载均衡的原则，优先选择新的节点存储，从而简化了系统扩容的管理，降低了操作成本。扩容时容量和性能支持线性扩展，最大可提供100PB规模的单一文件系统，真正实现“应需而变”，为业务的长期高性价比提供保障。

Ø 高性能存储设计

分布式存储系统通过全局缓存模式，整合所有节点缓存，容量最大可达55TB的虚拟缓存池。同一文件的数据在缓存池只缓存一份，任意节点均可命中，有效提高数据访问命中率，减少硬盘读写次数，降低访问时延，提升系统整体性能。

分布式存储系统支持10GE高速以太网，单个节点可提供4个10GE端口。还同时支持IB（InfiniBand）网络，带宽高达56Gbit/s，可满足更加苛刻的性能要求。可适配用户的不同组网需求，通过节点间高速互联，降低系统内部时延，提供卓越性能。

传统的NAS系统由引擎和存储单元组成。数据的全部并发访问需要先由NAS引擎处理，因此引擎容易成为性能瓶颈。而分布式存储系统采用全对称的逻辑架构，每个节点均可提供业务服务。通过负载均衡设计，数据访问在集群内均匀分布，可大幅提升系统并发访问能力。

Ø 高可靠存储设计

分布式存储系统采用Erasure Code保障数据一致性，确保海量数据存储场景下的数据安全可靠。Erasure Code是RAID的超集，能够支持比传统RAID算法更高的可靠性和更灵活的冗余策略，其设计思路是对文件进行原始分片（N份），通过纠错编码生成M个冗余校验块文件。写入时，由客户端进行切片（N）和转码（M），一共生成N+M份数据，存储系统自动从各存储节点中选择一个硬盘组成一个N+M个磁盘的磁盘组，分别写入N+M份数据。任意一份数据损坏的情况下，可以通过其它数据恢复，最大可以支持M份数据的损坏。

Ø 高可用存储设计

分布式存储系统支持可视化自动部署，提供了专门的网管管理工具，它支持WEB UI形式，可以方便的显示所有设备的组网结构，所有设备的容量，CPU，内存等信息，以及所有进程的运行状态，业务的运行状态，实时展示系统性能信息，保存和查询操作日志。管理维护高效集中，简洁易用。管理员可以通过集群内任意节点访问网管界面，完成对硬件、软件、集群和业务的统一管理。单个节点故障不影响网管的正常登录和使用。

分布式存储系统可实现对存储空间的细化管理。支持按照用户、用户组或目录的配额管理和配置，并且支持嵌套的配额管理，便于资源管理。分布式存储系统的配额管理特性包括：

l 基于目录的配额：管理员可以针对为空的共享目录设置配额，限制该目录的最大可用存储空间。

l 基于用户/用户组的配额：管理员可以对任意用户/用户组设置配额，限制该用户/用户组可以使用的最大可用存储空间。

#### 1.1.2.3 云存储系统物理拓扑

采用的是全对称、去中心化的分布式架构，系统内每个节点都能提供元数据服务（MDS）、数据服务（DS）以及外部访问的接口服务（CA），无独立元数据服务节点，消除性能瓶颈，不存在单点故障，在节点扩容、故障场景下都能无缝平滑切换，业务无感知。系统能够给应用服务器提供统一的文件系统空间，满足多台应用服务器之间共享数据的需求。云存储系统可以组建海量的存储资源池，容量分配不受物理硬盘数量的限制；并且存储容量可进行线性在线扩容，性能和容量的扩展都可以通过在线扩展完成。

![image-20210727160948085](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727160948085.png)

图6.92视频云存储物理拓扑图

#### 1.1.2.4 存储数据分类和存储空间计算

本次项目建设城市道路断面采集子系统、城市道路交叉口采集子系统、城市重点区域高点监控子系统，存放周期为90天。各部分存储空间计算方式如下：

Ø 城市道路断面采集子系统

该系统包含前端球机采集子系统、视频监控子系统、车牌识别子系统。

前端球机采集子系统码流为2Mbps，共计为323台；

视频监控子系统码流为6Mbps，共计为177台；

车牌识别子系统码流为10Mbps，共计162台。

存放90天需要的存储空间如下：

90*(323*2*86400/1024/1024/8+177*6*86400/1024/1024/8+162*10*86400/1024/1024/8)=3085TB

Ø 城市道路交叉口采集子系统

城市道路交叉口采集子系统共计184个，每个为2路摄像头，该系统仅采集基本数据，不存放视频和图片，数据量每秒小于1kbps，数据存放量为5年，存储数据量如下：

5*365*（184*2*1*86400/1024/1024/1024/8）=7TB

Ø 城市重点区域高点监控子系统

城市重点区域高点监控子系统为20个点，高点监控为拼接图像，码流为16Mbps，数据存储量如下：

90*(20*16*86400/1024/1024/8=297TB

综上可得，本次建设所需存储空间为3085+7+297=3389TB

##### 1. 建设方案

存储方面设计选用当前先进的NAS存储技术，全部设备都是基于IP网络进行数据存储。

云存储节点磁盘阵列支持36盘位，可以放置36块硬盘，硬盘选用SATA接口，容量为,6T的稳定性强的企业级硬盘，云存储采用EC算法，分布式云存储系统磁盘的综合利用率约为85%。

在实际部署过程中，考虑到系统内的负载均衡、故障替换等因素，在固有空间基础上需增加5%的系统冗余。

单台服务器的实际存储空间为：6*1012/10244*36*0.85*0.95=158TB。

##### 2. 网络带宽分析

根据本次项目建设的各个子系统码流，可以计算出汇聚的带宽如下：

Ø 城市道路断面采集子系统

前端球机采集子系统码流为2Mbps，共计为323台；

视频监控子系统码流为6Mbps，共计为177台；

车牌识别子系统码流为10Mbps，共计162台。

带宽容量=323*2+177*6+162*10=3328 Mb

Ø 城市道路交叉口采集子系统

数据流量非常小，可以忽略。

Ø 城市重点区域高点监控子系统

高点监控为拼接图像，码流为16Mbps，共计20个点。

带宽容量=20*16=320Mb

Ø 总的带宽容量=3328+320=3648Mb

##### 3. 云存储设计

智慧交通感知系统建设所需后台存储空间为3389TB。

单台服务器提供158TB的可用存储空间,因此，本项目所需的服务器数量为3389/158=22台。

**表**6.92**云存储设备需求表**

| **序号** | **设备类型**     | **技术参数要求**                                             | **单位** | **数量** |
| -------- | ---------------- | ------------------------------------------------------------ | -------- | -------- |
| 1        | 云存储服务       | 双路6核CPU，48G内存，36*6TB 7200转SATA盘，2*300GB SAS硬盘，冗余电源/风扇,2*双端口万兆网卡，基础管理软件，包含CIFS、NFS、NDMP、负载均衡、性能加速 | 台       | 22       |
|          |                  |                                                              |          |          |
| 2        | 云存储接入交换机 | 48*万兆SFP+,6*40G QSFP+,2*交流电源,端口侧出风                | 台       | 2        |
| 3        | 云存储管理交换机 | 48电口，2千兆SFP+,2*交流电源                                 | 台       | 2        |

**表**6.92**云存储设备需求表**

 

##### 4. 云存储技术规格

云存储采用全对称分布式架构，性能、容量随节点数增加而线性增加，最大支持288个节点。

全局命名空间，所有节点可组成一个完整的文件系统，应用可通过任意节点，任意前端网络访问文件系统内的数据。

单一文件系统存储容量可扩展至≥100PB

支持N+M冗余模式，最多可接受4个节点同时失效而不丢失数据；对象存储，最多可接受3个节点同时失效而不丢失数据

前后端网络隔离，必须独配置独立的后端网络接口卡和交换机承载内部流量

冗余性：组网全冗余部署，无单点故障。

系统支持全局缓存，提高存储访问效率，每节点的缓存可为全局所用。

具备对后端10GE交换机的管理能力：支持查询端口状态和告警转发。

支持NFS（V3/V4），SMB（V1/V2/V3），HDFS（支持与Cloudera对接），FTP，NDMP，Amazon S3/OpeanStack Swift接口。

支持单客户端对多个节点进行并发访问，单客户端最大带宽可达2.5GBps。

为了确保数据可靠性，恢复速率小于1TB/小时，单盘数据重构时间需小于6小时；

支持并配置客户端连接负载均衡软件,负载策略支持CPU占用率、网络带宽、TCP/IP连接数、轮询、节点能力值。

支持并配置视频监控图像修复功能。

处理器:高性能Intel 64位多核CPU

48G内存，2GB掉电保护内存。

主机接口:配置2个千兆以太网口，4个万兆。

冗余电源: 1+1电源冗余。

风扇: 支持1+1风扇冗余。

最大支持36个数据盘位。

硬盘:3.5寸 7200rpm，6TB以上容量。

## 1.2 数据库规划

### 1.2.1 Oracle数据库规划

### 1.2.2 数据对象的命名规范

1.通用规范

1) 使用英文：要用简单明了的英文单词，不要用拼音，特别是拼音缩写。主要目的很明确，让人容易明白这个对象是做什么用的。

2) 一律大写，特别是表名：有些数据库，表的命名乃至其他数据对象的命名是大小写敏感的，为了避免不必要的麻烦，并且尊重通常的习惯，最好一律用大写。

2.数据库对象命名规范

#### 1.2.2.1 表的命名

表名的前缀：前缀_表名_T。为表的名称增加一个或者多个前缀，前缀名不要太长，可以用缩写，最好用下划线与后面的单词分开；其目的有这样几个：

为了不与其他项目或者其他系统、子系统的表重名。

表示某种从属关系，比如表明是属于某个子系统、某个模块或者某个项目等等。表示这种从属关系的一个主要目的是，从表名能够大概知道如何去找相关的人员。比如以子系统为前缀的，当看到这个表的时候，就知道有问题可以去找该子系统的开发和使用人员。

Ø 视图命名：相关表名_V（或者根据需要另取名字）。

Ø 程序包命名：程序包名_PKG（用英文表达程序包意义）。

Ø 存储过程命名：存储过程名_PRO（用英文表达存储过程意义）。

Ø 函数命名：函数名称_FUN（用英文表达函数作用。

Ø 触发器命名：触发器名称_TRI（用英文表达触发器作用）。

Ø 索引命名：表名_字段名_IDX(如果存在多字段索引，取每字段前三个字符加下划线组合，如在 custom, cutting, curtail 上建立联合索引，命名为表名_cus_cut_cur_IDX,如果前三个截取字符相同，就从字段名称中不同的字符开始取三个字符加下划线组合，如在 custid, custom,custname上建立联合索引，就命名为表_tid_tom_tna_IDX。

Ø 唯一索引命名：表名_字段名_UNI(如果存在多字段唯一索引，取每字段前三个字符加下划线组合，如在 custom, cutting, curtail上建立唯一索引，命名为表名_ cus_cut_cur_UNI,如果前三个截取字符相同，就从字段名称中不同的字符开始取三个字符加下划线组合，如：在 custid, custom,custname上建立唯一索引，命名：表_tid_tom_tna_UNI。

Ø 主键命名：表名_字段名_PK(如果存在多字段主键，取每字段前三个字符加下划线组合，如在 custom, cutting, curtail上建立主键，命名为表名cus_cut_cur_PK,如果前三个截取字符相同，就从字段名称中不同的字符开始取三个字符加下划线组合，如在 custid, custom,custname上建立主键，命名：表_tid_tom_tna_PK。

Ø 外键命名：表名_主表名_字段名_FK。

Ø Sequence 命名：表名_列名_SEQ（或者根据需要另取名字）。

Ø Synonym 命名：与对应的数据库对象同名。

#### 1.2.2.2 数据库对象设计原则

Ø 表的设计

每个表，都必须要有主键。主键是每行数据的唯一标识，保证主键不可随意更新修改，在不知道是否需要主键的时候，请加上主键，它会为你的程序以及将来查找数据中的错误等等，提供一定的帮助。

一个表的某列与另一表有关联关系的时候，如果加得上的话，请加上外键约束。外键是很重要的，所以要特别强调。

适量建外键。为了保证外键的一致性，数据库会增加一些开销，如果有确凿的并且是对性能影响到无法满足用户需求的证据，可以考虑不建外键。否则，还是应该建外键。

不要以数据操作不方便为理由而不建外键。是的，加上外键以后，一些数据操作变得有些麻烦，但是这正是对数据一致性的保护。正是因为这种保护很有效，所以最好不要拒绝它。

以缺省的方式建立外键（即用delete restrict方式），以达到保护数据一致性的目的；外键在保护数据一致方面非常有效。如果不建外键，数据库中容易出现垃圾数据，并且无人知晓。当数据量很大的时候，查找这些垃圾数据也是相当困难的。而应用程序在设计时，往往没有考虑或者也无法照顾到垃圾数据。

Ø 列的设计

字段的宽度要在一定时间内足够用，但也不要过宽，占用过多的存储空间,对于长度不确定的列，采用可变长度的数据类型如 varchar类型。

字段的类型及宽度在设计以及后面进行开发时，往往要与应用的设计、开发人员商讨，以得到双方认可的类型及宽度。

除非必要，否则尽量不加冗余列。所谓冗余列，是指能通过其他列计算出来的列，或者是与某列表达同一含义的列，或者是从其他表复制过来的列等等。冗余列需要应用程序来维护一致性，相关列的值改变的时候，冗余列也需要随之修改，而这一规则未必所有人都知道，就有可能因此发生不一致的情况。如果是应用的特殊需要，或者是为了优化某些逻辑很复杂的查询等操作，可以加冗余列。

除非必要，否则尽量不使用LONG, TEXT, BLOB, CLOB, NCLOB, LONG, LONG RAW这一类的数据类型，而是使用其他可以替代的数据类型；优先使用varchar2类型替代CHAR类型，除非列宽有严格的要求而且得到应用严格支持。

Ø 记录数

单表的记录数一般控制在两千万条 (参考值，各应用可以根据实际情况进行适量调整) 以内。

记录数在两千万和两亿条之间的表一定要采用分区技术，并根据应用的使用情况创建合适的分区标准，单个分区内的记录数一般控制在两千万条(参考值，各应用可以根据实际情况进行适量调整)以内，同时表的索引使用对应的分区索引。

记录数超过两亿条的表一定要考虑信息生命周期，必须考虑历史数据的剥离，并在应用设计中完成对历史数据的相应处理功能（历史数据的剥离规则须经业务使用部门的确认）。

Ø 索引的设计

索引是从数据库中获取数据的最高效方式之一。95%的数据库性能问题都可以采用索引技术得到解决。但大量的DML操作会增加系统对索引的维护成本，对性能会有一定影响，对于插入相当频繁的表要慎重建索引，索引也会占相当的存储空间，所以要根据硬件环境和应用需求在空间和时间上达到最好的平衡点，主要原则：

适当利用索引提高查询速度：当数据量比较大，了解应用程序的会有哪些查询，依据这些查询需求建相应的索引；最好亲自试验一下，模拟一下生产环境的数据量，在此数据量下，比较一下建索引前后的查询速度；索引对性能会有一定影响，对于DML频繁列的索引要定期维护（重建）。但是，索引的结构对于索引的更新（比如在插入数据的时候）是有一定优化的，所以不要在没有试验以前过分夸大它对性能的影响。最终还是以试验为准。

不要建实际用不上的索引，与上条相关，如果建的索引并不提高任何一应用中的查询速度，则要把它删除；有些数据库有相关工具可以发现实际未被使用的索引，可以利用一下。

索引类型的选择：要根据数据分布及应用来决定如何建立索引，一般的高基数数据列（高基数数据列是指该列有很多不同的值）时 ,建立BTree索引（一般数据库索引的缺省类型）；当低基数数据列（该列有大量相同的值）时，可以考虑建立位图索引（如果所选数据库支持的话），但位图索引是压缩类型索引，所以DML（增、删、改）的代价更高，要综合考虑。

索引列的选择：如果检索条件有可能包含多列，创建联合主键或者联合索引，把最常用于检索条件的列放在最前端，其他的列排在后面；不要索引使用频繁的小型表，假如这些小表有频繁的DML就更不要建立索引，维护索引的代价远远高于扫描表的代价。

主键索引在建立的时候一定要明确的指定名称，不能让系统默认建立主键索引（可能有些数据库无法指定主键名，则例外）。

外键必须需建索引。当有一定数据量，并且经常以外键所在列为关联，进行关联查询时，需要建索引（可能有些数据库自动为外键建索引，则例外）。

当有联合主键或者联合索引时，注意不要建重复的索引。举例说明：

表EMPLOYEES，它的主键是建立在列DEPARTID和EMPLOYEEID上的联合主键，并且创建主键的语句中DEPARTID在前，EMPLOYEEID在后。在这样一个表里，通常就没有必要再为DEPARTID建一个索引了；联合索引的情况也一样。更复杂的情况，比如表EMPLOYEES，有一个索引建立在列CORPID, DEPARTID, EMPLOYEEID三列上，在创建语句中也依据上述顺序，就没有必要再为CORPID建立索引；也没有必要再建立以CORPID在前，DEPARTID在后的联合索引；如果EMPLOYEEID需要索引，那么为EMPLOYEEID建立一个索引是不与上面的索引重复的；DEPARTID列也类似。

控制一个表的索引数量，尽量使得一个表的索引数量小于五个。

Ø 视图的设计

在不太清楚视图用法的情况下，尽量不建。因为一旦建了，就有被滥用的危险。

如果需要建视图，只要是打算长期使用的，请写入数据库设计中。明确它的用途、目的。

建立视图时要明确写出所有要选择出的列名而不要以SELECT *来代替，可以使结构清晰可读性增强，也不会增加它对表的所有字段的依赖，而表是很可能修改的，特别是增加字段。就很有可能导致使用该视图的应用程序出错。

### 1.2.3 Hbase数据库规划

Hbase与RDBMS的区别在于：HBase的Cell（每条数据记录中的数据项）是具有版本描述的（versioned），行是有序的，列（qualifier）在所属列簇（Column families）存在的情况下，由客户端自由添加。以下的几个因素是Hbase Schema设计需要考虑的问题： 

1、 Hbase中没有joins的概念

大表的结构可以使得不需要joins，而解决这一问题。

 2、Row keys 设计

主键，在Region里按字母顺序来排序(byte数组存储)。

写入要分散，如订单表： order_id做逆排序后做rowkey，以便分布式存储，避免数据只保存在个别节点上。多条件查询时，设为组合row key

注：读取数据只能按row key（及其range）或scan全表扫描，确保查询高效

 3、列族CF设计

尽量少，建议CF数量在1.2个。设计Hbase schema的时候，要尽量只有一个column family。flush和compaction触发的基本单位都是Region级别。当一个CF有大量的数据的时候会触发整个region里面的其他CF的memstore（其实这些memstore可能仅有少量的数据，还不需要flush的）也发生flush动作；另外compaction触发的条件是当store file的个数（不是总的store file的大小）达到一定数量的时候会发生，而flush产生的大量store file通常会导致compaction，flush/compaction会发生很多IO相关的负载，这对Hbase的整体性能有很大影响，所以选择合适的column family个数很重要。

## 1.3 主机规划

根据服务器配置需求，具体的主机需求如下表：

**表**6.93**服务器需求**

| **序号** | **用途**           | **数量(台)** | **备注**                         |
| -------- | ------------------ | ------------ | -------------------------------- |
| 1        | 采集子系统服务器   | 4            | 采用智慧城市云计算中心服务器资源 |
| 2        | Hadoop集群服务器   | 10           |                                  |
| 3        | Storm集群服务器    | 5            |                                  |
| 4        | 应用服务器         | 4            |                                  |
| 5        | 统一消息服务服务器 | 2            |                                  |
| 6        | 数据库服务器       | 2            |                                  |

### 1.3.1 采集主机规划

采集主机用于实时的收取各个采集设备采集过来的数据，然后将数据上传至大数据计算中心，用于各个交通数据的计算。

**表**6.94**采集主机配置**

| **基本配置**       | **DELL PCSERVER R720**                                       |
| ------------------ | ------------------------------------------------------------ |
| 机箱配置           | 2.5" 机箱含高至8块硬盘                                       |
| 处理器             | 英特尔至强E5.2603 (1.80GHz,10M 高速缓存, 6.4GT/s QPI, No Turbo), 4C, 80W |
| 附加处理器         | 不含添加的处理器                                             |
| 内存DIMM类型与速度 | 1333 MHz RDIMMs                                              |
| 内存配置类型       | 性能优化                                                     |
| 内存容量           | 4GB RDIMM, 1333 MHz, 低电压, 单列, x4 带宽                   |
| RAID配置           | C1 . 无 Raid，用于 H310, 1.16 硬盘, 最大值取决于机箱         |
| RAID控制器         | PERC H310 集成 RAID 控制器                                   |
| 硬盘               | PERC H310 集成 RAID 控制器                                   |
| 硬盘（第二组）     | 无                                                           |
| 硬盘（第三组）     | 无                                                           |
| SATA硬盘信息       | 无                                                           |
| 操作系统           | Centos 7.0                                                   |

### 1.3.2 Hadoop集群主机规划

使用hadoop2.7.3版本配置Hadoop集群，同时配置NameNode+HA、ResourceManager+HA，并使用zookeeper来管理Hadoop集群。Hadoop节点数依据实际情况扩充。

表6.95**集群主机规划**

|       | **IP**          | **Namenode** | **Datanode** | **ResourceManger** | **Journalnode** | **Zookeeper** |
| ----- | --------------- | ------------ | ------------ | ------------------ | --------------- | ------------- |
| NODE  | 192.168.100.100 | Y            | N            | Y                  | Y               | Y             |
| NODE1 | 192.168.100.101 | Y            | N            | Y                  | Y               | Y             |
| NODE2 | 192.168.100.102 | N            | Y            | N                  | Y               | Y             |
| NODE3 | 192.168.100.103 | N            | Y            | N                  | Y               | Y             |
| NODE4 | 192.168.100.104 | N            | Y            | N                  | Y               | Y             |
| NODE5 | 192.168.100.105 | N            | Y            | N                  | Y               | Y             |
| NODE6 | 192.168.100.106 | N            | Y            | N                  | Y               | Y             |
| NODE7 | 192.168.100.107 | N            | Y            | N                  | Y               | Y             |
| NODE8 | 192.168.100.108 | N            | Y            | N                  | Y               | Y             |
| NODE9 | 192.168.100.109 | N            | Y            | N                  | Y               | Y             |

### 1.3.3 Storm集群主机规划

1) Strom 集群主机规划

**表**6.96**storm集群主机规划**

| **IP地址**      | **主机名**        | **角色**   |
| --------------- | ----------------- | ---------- |
| 192.168.100.111 | strom_zk1         | leader     |
| 192.168.100.112 | strom_zk2         | follower   |
| 192.168.100.118 | storm_nimbus      | Nimbus     |
| 192.168.100.119 | strom_supervisor1 | Supervisor |
| 192.168.100.120 | strom_supervisor2 | Supervisor |

### 1.3.4 统一消息服务平台主机规划

统一消息服务平台主机用于对外提供交通公共信息的查询和发布，为一台应用服务器。

**表**6.97**统一消息服务主机配置**

| **基本配置**       | **DELL PCSERVER R720**                                       |
| ------------------ | ------------------------------------------------------------ |
| 机箱配置           | 2.5" 机箱含高至8块硬盘                                       |
| 处理器             | 英特尔至强E5.2603 (1.80GHz,10M 高速缓存, 6.4GT/s QPI, No Turbo), 4C, 80W |
| 附加处理器         | 不含添加的处理器                                             |
| 内存DIMM类型与速度 | 1333 MHz RDIMMs                                              |
| 内存配置类型       | 性能优化                                                     |
| 内存容量           | 64GB RDIMM, 1333 MHz, 低电压, 单列, x4 带宽                  |
| RAID配置           | C1 . 无 Raid，用于 H310, 1.16 硬盘, 最大值取决于机箱         |
| RAID控制器         | PERC H310 集成 RAID 控制器                                   |
| 硬盘               | PERC H310 集成 RAID 控制器                                   |
| 硬盘（第二组）     | 无                                                           |
| 硬盘（第三组）     | 无                                                           |
| SATA硬盘信息       | 无                                                           |
| 操作系统           | Centos 7.0                                                   |

## 1.4 云平台建设

传统的数据中心建设模式和方案已经不再能满足建设需求，本项目拟采用云计算技术进行资源池化整合，通过IT系统基础设施逐步云化部署，基于云平台虚拟化技术，实现计算存储网络的虚拟化、资源共享、灵活分配，实现业务服务器的整合和调配，集中化以及基于策略的管理，以适应快速发展的业务需求，降低IT总持有成本，聚焦核心业务发展。

本系统建设采用云数据中心的思路，利用云平台统一设备管理，低维护成本，易扩展性，资源部署周期短等优势，把投入到传统IT建设与维护的核心资源释放出来，聚焦产业信息管理。利用云平台易扩展、设备易替换，提高资源复用率，避免传统IT烟囱式发展，资源利用率低的现状，能够有效地实现节能减排。通过云平台HA、热迁移功能，能够有效减少设备故障时间，确保核心业务的连续性，避免传统IT单点故障导致的业务不可用。

### 1.4.1 云平台总体架构

云计算的本质是构建共享资源池，通过资源统一管理、自动化管理和自服务管理，实现资源复用和弹性，提高资源利用率，实现IT资源的服务化。对于云平台，建议构建一个多业务共享的资源池，通过资源的统一管理，实现资源的统一发放和自动化运维管理。

多个业务系统共享资源池，承载多个部门的业务系统，每个业务系统还会涉及到不同种类和不同安全级别需求。因此，新建资源池需要考虑不同业务系统对于资源在性能、特性、安全等方面的差异化需求，新建资源池，还需要考虑各个业务部门对于所属资源的必要的运维能力，以提高业务部门对于资源的能动性，同时降低集中管理的压力。建议采用虚拟数据中心（VDC），基于共享资源池为各个业务部门或业务系统分配专门的逻辑资源池，并实现自服务的运维管理。VDC服务可以匹配现有的运维组织架构，实现业务分散多资源池向多业务共享资源池的平滑过渡。

云数据中心资源池物理架构图：

![image-20210727161040429](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727161040429.png)

图6.93云数据中心资源池物理架构图

l 2台高性能一体化刀箱，各配置10块刀片节点运行虚拟化平台部署应用云化；

l 2套24端口光纤交换机组建本地冗余SAN网络，提高网络可靠性；

l 1套高端存储组成存储系统，与主机层应用虚拟化集群结合，保障核心业务高可用。

### 1.4.2 云平台资源池设计

本项目云平台资源池提供服务器、存储、网络的虚拟化功能，并向上对云管理系统提供接口。每套虚拟化引擎应该由一对主备管理节点组成。一对主备管理节点对应一个物理集群。一个物理集群中可以把多台服务器划分成一个资源集群，一个计算资源池有相同的调度策略，一个物理集群中可以包含多个资源集群。

![image-20210727161049172](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727161049172.png)

图6.94云平台资源池示意图

虚拟化引擎主要负责硬件资源的虚拟化，以及对虚拟资源、业务资源、用户资源的集中管理。它采用虚拟计算、虚拟存储、虚拟网络等技术，完成计算资源、存储资源、网络资源的虚拟化。同时通过统一的接口，对这些虚拟资源进行集中调度和管理，从而降低业务的运行成本，保证系统的安全性和可靠性，构筑安全、绿色、节能的云数据中心能力。云虚拟化软件虚拟化引擎有以下技术特点：

Ø 统一虚拟化平台

虚拟化引擎采用虚拟化管理软件，将计算资源划分为多个虚拟机资源，为用户提供高性能、可运营、可管理的虚拟机。

Ø 支持虚拟机资源按需分配。

QoS保证资源分配，隔离用户间影响。

支持多操作系统，包括Windows、Linux 32/64位操作系统以及系统之上的各种应用，包括Windows Server 2003 /2008 R2及以上版本服务器操作系统，Windows XP、Windows 7操作系统，Redhat、SUSE、CentOS、中标普华、Ubuntu、Fedora等多个发行版本的Linux操作系统。

Ø 兼容多种硬件设备

兼容异构的软硬件平台，屏蔽不同硬件差异，支持当前主流厂商的x86服务器产品以用主流存储设备，资源的更换升级对用户零感知。设备自动发现，资源快速发放，缩短业务上线时间。

Ø 大集群资源池

支持多资源池的管理和调度，支持计算集群的构建，可以针对不同的资源池设置不同的调度方式，提供灵活的负载均衡和HA策略。通过大集群可以避免资源池碎片过大，提升物理主机利用率；单个集群数量需支持128台主机。

Ø 自动化调度

虚拟化引擎支持自定义的资源管理SLA（Service-Level Agreement）策略、故障判断标准及恢复策略。

通过IT资源调度、热管理、能耗管理等一体化拉通，降低维护成本。

自动检测服务器或业务的负载情况，对资源进行智能调度，均衡各服务器及业务系统负载，保证系统良好的用户体验和业务系统的最佳响应。

Ø 完善的权限管理

虚拟化引擎可根据不同的角色、权限等，支持三员分立管理，提供完善的权限管理功能，授权用户对系统内容的资源进行管理。

Ø 精细化统计

针对不同的业务类型，进行精确统计。

按IT资源（CPU、内存、存储）用量统计。

按时统计。

Ø 云安全

虚拟化引擎采用多种安全措施和策略，并遵从信息安全法律法规，对用户接入、管理维护、数据、网络、虚拟化等提供端到端的业务保护。

### 1.4.3 虚拟数据中心VDC设计

本次云数据中心建设思路是在服务器、存储、网络等物理资源基础之上，通过云化的技术，将一个物理数据中心资源逻辑隔离成多个虚拟数据中心VDC，通过VDC计算灵活管理资源的逻辑抽象，对可用的 CPU 、内存资源、存储和网络进行管理。在云上的所有虚拟资产都被放置在多个“虚拟数据中心VDC”内。VDC的管理员可以设定不同的虚拟数据中心，虚拟数据中心间默认相互隔离。

![image-20210727161059292](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727161059292.png)

![image-20210727161103917](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727161103917.png)

图6.95云数据中心VDC设计图

虚拟数据中心机制实现了一个数据中心在逻辑上当个使用，同时满足集中运维的要求，也将资源池的维护和使用彻底解耦。

Ø 统一资源池层

在统一的物理资源池基础上，通过虚拟化平台，将物理分散的计算、存储、网络设备纳入统一资源池，供上层业务按需调度。虚拟化的资源池的划分和底层物理设备位置无任何关联，可以根据业务需要再次进行划分资源池。

Ø 资源使用层

在虚拟化后的资源池基础上，根据业务部门的不同需求，可以将资源再次进行细化划分，为每个业务部门分配相对独立的虚拟私有云（VPC）。虚拟私有云能够为业务部门提供安全、隔离的网络环境，实现各业务部门间应用的隔离，及外部网络访问的安全控制。

在各业务部门的VPC中，拥有独立的网络边界：虚拟防火墙，VPN，NAT；能够部署独立的内部网络能力：多平面，地址管理，DHCP，安全组，负载均衡器等。各业务部门利用VPC可以实现快速的业务部署。

#### 1.4.3.1 VDC架构优势

云平台以虚拟数据中心的形式为内/外部组织提供资源。通过以逻辑方式将计算、存储和网络容量组合成虚拟数据中心资源池，可以利用在计算中心服务的交付和提供环节之间的完全抽象化，更高效地管理资源。

不再是为组织提供孤立的多个物理基础架构，而是基于公用物理基础架构提供相互隔离的虚拟数据中心。通过将这些后端物理资源创建为资源池，硬件利用率和整合率都获得了提高。

![image-20210727161113451](https://gitee.com/er-huomeng/l-img/raw/master/img/image-20210727161113451.png)

图6.96 VDC架构图

#### 1.4.3.2 VDC规划设计

组织VDC就是组织的虚拟机数据中心，它是灵活管理该组织下资源的逻辑抽象，用于对可用的 CPU 、内存资源、存储和网络进行管理。组织VDC为系统管理员提供一种能力，用来良好的管理企业在云上的资产。企业用户或组织在云上的所有虚拟资产都被放置在一个或多个“虚拟数据中心”内。不同的组织可以设定不同的虚拟数据中心，虚拟数据中心间默认相互隔离。

虚拟数据中心管理如下信息：

l 组织，组织内的用户。通过设定组织和用户，允许一个组织内的多个用户访问同一个虚拟数据中心，从而实现资源共享使用。

l 资源配额。该组织可以使用的资源的最大容量，比如计算能力，内存，网络带宽，VLAN个数等等。

l 组织资产。该组织已经拥有的所有资产。

l 组网。资产之间的网络连接拓扑关系，以虚拟私有云的形式显示。

l 应用。该组织所拥有的应用信息，以虚拟应用的形式显示。

通过这个能力，可以让系统管理员良好的管理企业的资产。

组织VDC规划的原则是：每个业务部门分配一个独立的VDC,资源分配采用配额管控模式，事先为每个业务部门设定配额，管理员在配额范围内自主审批内部的资源申请。配额不足时，组织管理员需要向资产管理员申请。

组织可将资源提供给一组用户，并设置确定用户如何消耗这些资源的策略。可为要求拥有自己资源和/或策略的每组用户创建一个组织。

### 1.4.4 存储资源池设计

初期建设1套高端存储，配置双控，支持多控扩展；单套存储初期提供100TB以上的可用容量。

高端存储的关键技术要求包括：

l 支持双控到十六控线性扩展，满足高可靠、易扩展需求；

l 采用SAN/NAS 一体化设计，不再需要 NAS 网关设备，一套软硬件同时支持 SAN 和 NAS。NAS 与 SAN 一样，同样支持十六控的 Scale-out；

l 支持服务质量控制（QoS），避免多业务系统之间I/O的互相影响，保证核心业务系统的快速响应和服务质量；

l 支持缓存的分区，实现关键和非关键业务的区分对待，优先保障关键业务的性能，同时与服务质量控制（QoS）相配合，从而达到更好的服务效果；

l 支持异构虚拟化功能，可对主流厂家存储设备进行统一管理和分配，降低用户管理不同异构阵列的复杂度；

l 支持阵列双活架构，免网关部署双活存储系统，保障存储高可靠，实现业务连续；

l 支持国际通用的标准网络存储协议和应用开放协议，与主流服务器、主流操作系统之间保持良好的兼容性；

l 支持WEB管理方式或集中管理方式，支持简体中文，通俗易懂，维护操作方便、简单。

基于以上建设需求，建议选型高端存储产品作为本次云数据中心的核心存储，为核心业务提供最高水平的数据服务。

针对目前云数据中心的容量规划，配置1套高端阵列，高端存储架构须采用业界领先的智能矩阵SmartMatrix 2.0系统架构，支持免网关A-A双活功能，支持双控至十六控制器的线性扩展，控制器之间采用 PCIe 光互联设计，实现控制器间的业务交换。本项目每套存储配置如下：2个以上冗余控制器，支持Active/Active负载均衡，配置1TB以上大容量缓存，主机接口采用模块化设计，配置8个8Gb FC接口，配置10块900GB SSD硬盘，14块企业级1800GB 10000rpm高速SAS磁盘，配置32块企业级4TB 7200rpm NL SAS磁盘。同时配置以下多种高级功能特性：

l 配置RAID2.0+块虚拟化技术，实现故障的快速重构、负载均衡，提升系统可靠性，最大化硬盘资源利用率，提升存储管理效率。

l 配置缓存分区功能：通过给不同级别的用户配置不同大小的缓存分区，系统将保证该分区所占用的缓存容量，从而保证位于该分区中的业务应用的服务性能。通过限制提供给非关键应用的缓存资源，向关键应用提供更多的缓存资源，保障关键应用的性能，确保业务可靠性。

l 配置服务质量Qos功能：允许用户根据应用程序数据的一系列特征（IOPS、占用带宽等）对特定应用程序设置特定的性能目标。存储系统根据设定的性能目标，动态分配存储系统的资源来满足特定应用程序的服务级别要求，优先满足关键性应用程序服务级别的需求。

l 配置多租户功能：多租户技术可以让当前的存储系统为多个租户提供存储服务，并且在共用存储资源的同时隔离租户间的业务访问和管理。为了保证租户间业务的独立性，单个存储系统被划分成了多个安全并且相互隔离的分区，一套物理存储系统可以划分出多个租户区，租户间具有相互独立的存储资源。

l 配置自动精简配置功能：提升存储资源利用率，存储系统不会一次性分配所有的物理存储空间给精简LUN，其实际容量只有初始容量，后续将根据写入数据量实时按需分配物理存储空间。

l 配置自动分级存储功能：提供智能化数据存储管理的功能。该功能统计和分析数据的活跃度，将不同活跃度的数据和不同特点的存储介质动态匹配。通过数据迁移将活跃度高的繁忙数据迁移至具有更高性能的存储介质（如SSD硬盘），将活跃度低的空闲数据迁移至具有更高容量且更低容量成本的存储介质（如NL-SAS硬盘）。统计、分析和迁移活动基于该分层功能的实现策略和数据的性能要求。在统计、分析、迁移活动期间，不会对现有业务连续性和数据可用性造成影响。

l 配置SSD智能缓存功能：利用SSD盘对随机小I/O读取速度快的特点，将SSD盘组成智能缓存池，将访问频率高的随机小I/O读热点数据从传统的机械硬盘移动到由SSD盘组成的高速智能缓存池中。由于SSD盘的数据读取速度远远高于机械硬盘，所以该特性可以缩短热点数据的响应时间，从而提升系统的性能。

l 配置数据销毁功能：实现对关键涉密数据的安全销毁功能，保障数据安全擦除。当不再需要某个LUN时，您可以对该LUN进行数据销毁。执行该操作后，被删除的数据将无法进行恢复，以保证数据的安全性。

l 配置系统报表功能：高端存储系统的性能分析工具，采用数据采集、归档、分析、预测等方法，提供实时监控、趋势分析等功能，让用户能够更加直观、简单的了解存储系统的性能并可及时做出调整。

### 1.4.5 配置清单

**表**6.98**云平台配置清单**

| **序号** | **设备类型**     | **配置描述**                                                 | **单位** | **数量** |
| -------- | ---------------- | ------------------------------------------------------------ | -------- | -------- |
| 1        | 虚拟化刀箱       | 12U刀箱，冗余管理模块，6*3000W交流电源，满配风扇模块。配置冗余FC交换模块，对外可提供16个FC接口，配置冗余10GE交换模块，对外提供32个10GE接口，配置8个8G FC多模模块及8个10GE多模模块 | 台       | 2        |
| 2        | 虚拟化刀片服务器 | 刀片式服务器，2*E5-2630 V4,256G DDR4内存，2*300G 10K SAS硬盘， RAID卡（支持RAID0、1、10），2*10GE+2*8G FC | 台       | 20       |
| 3        | 高端存储         | 双控，1TB缓存，8*8G FC接口，10*900G SSD硬盘，14*1.8T 10K SAS硬盘，32*4TB NL SAS硬盘，自动精简配置、缓存分区、智能缓存、数据销毁、多租户、自动分层、系统报表许可，3年7x24x4H金牌+服务 | 台       | 1        |
| 4        | 光纤交换机       | 24端口光纤交换机，16端口激活，含16个8G FC多模模块，冗余电源  | 台       | 2        |
| 5        | 云平台软件       | 40颗CPU虚拟化高级版授权许可，含云平台管理软件，含3年维保     | 套       | 1        |
| 6        | 汇聚交换机       | 48*万兆SFP+,6*40G QSFP+,2*交流电源,端口侧出风，配置48个万兆光模块、1根QSFP+-40G-高速电缆 | 台       | 2        |

 